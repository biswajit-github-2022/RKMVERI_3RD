{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62f8a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------\n",
    "Transformer using pytorch and numpy\n",
    "-----------------------------------------------------------------------------\n",
    "AUTHOR: Soumitra Samanta (soumitra.samanta@gm.rkmvu.ac.in)\n",
    "-----------------------------------------------------------------------------\n",
    "Package required:\n",
    "Numpy: https://numpy.org/\n",
    "Matplotlib: https://matplotlib.org\n",
    "-----------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "import math\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d4aa92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Self-attention layer models is: \n",
      "self_attention_layer(\n",
      "  (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Self-attention layer input size: torch.Size([5, 100, 10])\n",
      "Self-attention layer output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer block models is: \n",
      "transformer_block_encoder(\n",
      "  (attention_): self_attention_layer(\n",
      "    (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (layer_norm1_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffnn_): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      "  (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer block models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]             110\n",
      "            Linear-2              [-1, 100, 10]             110\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "self_attention_layer-4              [-1, 100, 10]               0\n",
      "         LayerNorm-5              [-1, 100, 10]              20\n",
      "            Linear-6            [-1, 100, 1024]          11,264\n",
      "              ReLU-7            [-1, 100, 1024]               0\n",
      "            Linear-8              [-1, 100, 10]          10,250\n",
      "         LayerNorm-9              [-1, 100, 10]              20\n",
      "================================================================\n",
      "Total params: 21,884\n",
      "Trainable params: 21,884\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 1.62\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 1.70\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer block input size: torch.Size([5, 100, 10])\n",
      "Transformer block output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer encoder models is: \n",
      "transformer_encoder(\n",
      "  (trs_endr_blocks_): ModuleList(\n",
      "    (0-1): 2 x transformer_block_encoder(\n",
      "      (attention_): self_attention_layer(\n",
      "        (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "      )\n",
      "      (layer_norm1_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffnn_): Sequential(\n",
      "        (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "      )\n",
      "      (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer encoder models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]             110\n",
      "            Linear-2              [-1, 100, 10]             110\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "self_attention_layer-4              [-1, 100, 10]               0\n",
      "         LayerNorm-5              [-1, 100, 10]              20\n",
      "            Linear-6            [-1, 100, 1024]          11,264\n",
      "              ReLU-7            [-1, 100, 1024]               0\n",
      "            Linear-8              [-1, 100, 10]          10,250\n",
      "         LayerNorm-9              [-1, 100, 10]              20\n",
      "transformer_block_encoder-10              [-1, 100, 10]               0\n",
      "           Linear-11              [-1, 100, 10]             110\n",
      "           Linear-12              [-1, 100, 10]             110\n",
      "           Linear-13              [-1, 100, 10]             110\n",
      "self_attention_layer-14              [-1, 100, 10]               0\n",
      "        LayerNorm-15              [-1, 100, 10]              20\n",
      "           Linear-16            [-1, 100, 1024]          11,264\n",
      "             ReLU-17            [-1, 100, 1024]               0\n",
      "           Linear-18              [-1, 100, 10]          10,250\n",
      "        LayerNorm-19              [-1, 100, 10]              20\n",
      "transformer_block_encoder-20              [-1, 100, 10]               0\n",
      "================================================================\n",
      "Total params: 43,768\n",
      "Trainable params: 43,768\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 3.25\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 3.42\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer encoder input size: torch.Size([5, 100, 10])\n",
      "Transformer encoder output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Cross-attention layer models is: \n",
      "cross_attention_layer(\n",
      "  (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Cross-attention layer input size: torch.Size([5, 100, 10])\n",
      "Cross-attention layer output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer decoder block models is: \n",
      "transformer_block_decoder(\n",
      "  (attention_): self_attention_layer(\n",
      "    (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (cross_attention_): cross_attention_layer(\n",
      "    (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "    (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  )\n",
      "  (layer_norm1_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffnn_): Sequential(\n",
      "    (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "  )\n",
      "  (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder block models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]             110\n",
      "            Linear-2              [-1, 100, 10]             110\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "self_attention_layer-4              [-1, 100, 10]               0\n",
      "         LayerNorm-5              [-1, 100, 10]              20\n",
      "           Dropout-6              [-1, 100, 10]               0\n",
      "            Linear-7              [-1, 100, 10]             110\n",
      "            Linear-8              [-1, 100, 10]             110\n",
      "            Linear-9              [-1, 100, 10]             110\n",
      "cross_attention_layer-10              [-1, 100, 10]               0\n",
      "        LayerNorm-11              [-1, 100, 10]              20\n",
      "          Dropout-12              [-1, 100, 10]               0\n",
      "           Linear-13            [-1, 100, 1024]          11,264\n",
      "             ReLU-14            [-1, 100, 1024]               0\n",
      "           Linear-15              [-1, 100, 10]          10,250\n",
      "        LayerNorm-16              [-1, 100, 10]              20\n",
      "          Dropout-17              [-1, 100, 10]               0\n",
      "================================================================\n",
      "Total params: 22,234\n",
      "Trainable params: 22,234\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.81\n",
      "Forward/backward pass size (MB): 1.68\n",
      "Params size (MB): 0.08\n",
      "Estimated Total Size (MB): 5.58\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer block input size: torch.Size([5, 100, 10])\n",
      "Transformer block output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer decoder models is: \n",
      "transformer_decoder(\n",
      "  (trs_dcdr_blocks_): ModuleList(\n",
      "    (0-1): 2 x transformer_block_decoder(\n",
      "      (attention_): self_attention_layer(\n",
      "        (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "      )\n",
      "      (cross_attention_): cross_attention_layer(\n",
      "        (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "        (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      "      )\n",
      "      (layer_norm1_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm3_): LayerNorm((10,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffnn_): Sequential(\n",
      "        (0): Linear(in_features=10, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=10, bias=True)\n",
      "      )\n",
      "      (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1              [-1, 100, 10]             110\n",
      "            Linear-2              [-1, 100, 10]             110\n",
      "            Linear-3              [-1, 100, 10]             110\n",
      "self_attention_layer-4              [-1, 100, 10]               0\n",
      "         LayerNorm-5              [-1, 100, 10]              20\n",
      "           Dropout-6              [-1, 100, 10]               0\n",
      "            Linear-7              [-1, 100, 10]             110\n",
      "            Linear-8              [-1, 100, 10]             110\n",
      "            Linear-9              [-1, 100, 10]             110\n",
      "cross_attention_layer-10              [-1, 100, 10]               0\n",
      "        LayerNorm-11              [-1, 100, 10]              20\n",
      "          Dropout-12              [-1, 100, 10]               0\n",
      "           Linear-13            [-1, 100, 1024]          11,264\n",
      "             ReLU-14            [-1, 100, 1024]               0\n",
      "           Linear-15              [-1, 100, 10]          10,250\n",
      "        LayerNorm-16              [-1, 100, 10]              20\n",
      "          Dropout-17              [-1, 100, 10]               0\n",
      "transformer_block_decoder-18              [-1, 100, 10]               0\n",
      "           Linear-19              [-1, 100, 10]             110\n",
      "           Linear-20              [-1, 100, 10]             110\n",
      "           Linear-21              [-1, 100, 10]             110\n",
      "self_attention_layer-22              [-1, 100, 10]               0\n",
      "        LayerNorm-23              [-1, 100, 10]              20\n",
      "          Dropout-24              [-1, 100, 10]               0\n",
      "           Linear-25              [-1, 100, 10]             110\n",
      "           Linear-26              [-1, 100, 10]             110\n",
      "           Linear-27              [-1, 100, 10]             110\n",
      "cross_attention_layer-28              [-1, 100, 10]               0\n",
      "        LayerNorm-29              [-1, 100, 10]              20\n",
      "          Dropout-30              [-1, 100, 10]               0\n",
      "           Linear-31            [-1, 100, 1024]          11,264\n",
      "             ReLU-32            [-1, 100, 1024]               0\n",
      "           Linear-33              [-1, 100, 10]          10,250\n",
      "        LayerNorm-34              [-1, 100, 10]              20\n",
      "          Dropout-35              [-1, 100, 10]               0\n",
      "transformer_block_decoder-36              [-1, 100, 10]               0\n",
      "================================================================\n",
      "Total params: 44,468\n",
      "Trainable params: 44,468\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 3.81\n",
      "Forward/backward pass size (MB): 3.37\n",
      "Params size (MB): 0.17\n",
      "Estimated Total Size (MB): 7.35\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder input size: torch.Size([5, 100, 10])\n",
      "Transformer decoder output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class self_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Self attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Self attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dims_embd_ = dims_embd\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor \n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the self attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        y = []\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        Q=self.W_q_(x)\n",
    "        K=self.W_k_(x)\n",
    "        V=self.W_v_(x)\n",
    "\n",
    "        temp = torch.bmm(Q, K.transpose(1, 2))\n",
    "        temp=temp/(self.dims_embd_) ** (1/2.)\n",
    "        temp = F.softmax(temp, dim=2)\n",
    "\n",
    "        y = torch.bmm(temp, V)\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    \n",
    "class transformer_block_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        x=self.layer_norm1_(x + self.attention_(x))\n",
    "        x=self.layer_norm2_(x + self.ffnn_(x))\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class transformer_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_encoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer encoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_encoder (int):    Number encoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_endr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_encoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_encoder_ = num_layers_encoder\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        for i in range(self.num_layers_encoder_):\n",
    "            x = self.trs_endr_blocks_[i](x)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class cross_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Cross attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "\n",
    "        # self.mask=  torch.tril(torch.ones())\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the cross-attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        Q = self.W_q_(x)\n",
    "        K = self.W_k_(y)\n",
    "        V = self.W_v_(y)\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        temp = torch.bmm(Q, K.transpose(1, 2))\n",
    "        temp = temp / (self.dims_embd_ ** 0.5)\n",
    "        temp = F.softmax(temp, dim=-1)\n",
    "\n",
    "        y = torch.bmm(temp, V)\n",
    "\n",
    "\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    \n",
    "\n",
    "class transformer_block_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single decoder block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        self.cross_attention_ = cross_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm3_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        x = self.layer_norm1_(x + self.attention_(x))\n",
    "        x = self.droput_ops_(x)\n",
    "        x = self.layer_norm2_(x + self.cross_attention_(x, y))\n",
    "        x = self.droput_ops_(x)\n",
    "        x = self.layer_norm3_(x + self.ffnn_(x))\n",
    "        x = self.droput_ops_(x)\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class transformer_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_decoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer decoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_decoder (int):    Number decoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_dcdr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_decoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_decoder_ = num_layers_decoder\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        for block in self.trs_dcdr_blocks_:\n",
    "            x = block(x, y)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "  def __init__(self, d_model: int, dropout: float = 0.1, max_length: int = 5000):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      d_model:      dimension of embeddings\n",
    "      dropout:      randomly zeroes-out some of the input\n",
    "      max_length:   max sequence length\n",
    "    \"\"\"\n",
    "    # inherit from Module\n",
    "    super().__init__()     \n",
    "\n",
    "    # initialize dropout                  \n",
    "    self.dropout = nn.Dropout(p=dropout)      \n",
    "\n",
    "    # create tensor of 0s\n",
    "    pe = torch.zeros(max_length, d_model)    \n",
    "\n",
    "    # create position column   \n",
    "    k = torch.arange(0, max_length).unsqueeze(1)  \n",
    "\n",
    "    # calc divisor for positional encoding \n",
    "    div_term = torch.exp(                                 \n",
    "            torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model)\n",
    "    )\n",
    "\n",
    "    # calc sine on even indices\n",
    "    pe[:, 0::2] = torch.sin(k * div_term)    \n",
    "\n",
    "    # calc cosine on odd indices   \n",
    "    pe[:, 1::2] = torch.cos(k * div_term)  \n",
    "\n",
    "    # add dimension     \n",
    "    pe = pe.unsqueeze(0)          \n",
    "\n",
    "    # buffers are saved in state_dict but not trained by the optimizer                        \n",
    "    self.register_buffer(\"pe\", pe)                        \n",
    "\n",
    "  def forward(self, x: Tensor):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "      x:        embeddings (batch_size, seq_length, d_model)\n",
    "    \n",
    "    Returns:\n",
    "                embeddings + positional encodings (batch_size, seq_length, d_model)\n",
    "    \"\"\"\n",
    "    # add positional encoding to the embeddings\n",
    "    x = x + self.pe[:, : x.size(1)].requires_grad_(False) \n",
    "\n",
    "    # perform dropout\n",
    "    return self.dropout(x)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "dims_embd = 10\n",
    "num_data_points = 100\n",
    "batch_size = 5\n",
    "num_hidden_nodes_ffnn = 1024\n",
    "dropout_prob = 0.2\n",
    "num_layers_encoder = 2\n",
    "\n",
    "x = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "y = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "\n",
    "emb = nn.Linear(dims_embd, dims_embd)\n",
    "x=emb(x)\n",
    "y=emb(y)\n",
    "\n",
    "\n",
    "p=PositionalEncoding(dims_embd,dropout_prob,num_data_points)\n",
    "\n",
    "x=p(x)\n",
    "\n",
    "\n",
    "y=p(y)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Test Self-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_self_attention_layer = self_attention_layer(dims_embd)\n",
    "print('Self-attention layer models is: \\n{}' .format(model_self_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_self_attention_layer(x)\n",
    "print('Self-attention layer input size: {}' .format(x.shape))\n",
    "print('Self-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "        \n",
    "# Test Transformer encoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_encoder = transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer block models is: \\n{}' .format(model_transformer_block_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_encoder(x)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer encoder input output size \n",
    "print('='*70)\n",
    "model_transformer_encoder = transformer_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer encoder models is: \\n{}' .format(model_transformer_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer encoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_encoder(x)\n",
    "print('Transformer encoder input size: {}' .format(x.shape))\n",
    "print('Transformer encoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# # Test Cross-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_cross_attention_layer = cross_attention_layer(dims_embd)\n",
    "print('Cross-attention layer models is: \\n{}' .format(model_cross_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_cross_attention_layer(x, y)\n",
    "print('Cross-attention layer input size: {}' .format(x.shape))\n",
    "print('Cross-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer decoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_decoder = transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer decoder block models is: \\n{}' .format(model_transformer_block_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_decoder(x, y)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer decoder input output size \n",
    "print('='*70)\n",
    "model_transformer_decoder = transformer_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer decoder models is: \\n{}' .format(model_transformer_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_decoder(x, y)\n",
    "print('Transformer decoder input size: {}' .format(x.shape))\n",
    "print('Transformer decoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fe3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
