{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f8a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------\n",
    "Transformer using pytorch and numpy\n",
    "-----------------------------------------------------------------------------\n",
    "AUTHOR: Soumitra Samanta (soumitra.samanta@gm.rkmvu.ac.in)\n",
    "-----------------------------------------------------------------------------\n",
    "Package required:\n",
    "Numpy: https://numpy.org/\n",
    "Matplotlib: https://matplotlib.org\n",
    "-----------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# import torch\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4aa92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Self-attention layer models is: \n",
      "self_attention_layer(\n",
      "  (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Self-attention layer input size: torch.Size([5, 256, 1000])\n",
      "Self-attention layer output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer block models is: \n",
      "transformer_block_encoder(\n",
      "  (attention_): self_attention_layer(\n",
      "    (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  )\n",
      "  (layer_norm1_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffnn_): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      "  (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer block models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 256, 1000]       1,001,000\n",
      "            Linear-2            [-1, 256, 1000]       1,001,000\n",
      "            Linear-3            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-4            [-1, 256, 1000]               0\n",
      "         LayerNorm-5            [-1, 256, 1000]           2,000\n",
      "            Linear-6            [-1, 256, 1024]       1,025,024\n",
      "              ReLU-7            [-1, 256, 1024]               0\n",
      "            Linear-8            [-1, 256, 1000]       1,025,000\n",
      "         LayerNorm-9            [-1, 256, 1000]           2,000\n",
      "================================================================\n",
      "Total params: 5,057,024\n",
      "Trainable params: 5,057,024\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.98\n",
      "Forward/backward pass size (MB): 17.67\n",
      "Params size (MB): 19.29\n",
      "Estimated Total Size (MB): 37.94\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer block input size: torch.Size([5, 256, 1000])\n",
      "Transformer block output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer encoder models is: \n",
      "transformer_encoder(\n",
      "  (trs_endr_blocks_): ModuleList(\n",
      "    (0-1): 2 x transformer_block_encoder(\n",
      "      (attention_): self_attention_layer(\n",
      "        (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "      )\n",
      "      (layer_norm1_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffnn_): Sequential(\n",
      "        (0): Linear(in_features=1000, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "      )\n",
      "      (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer encoder models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 256, 1000]       1,001,000\n",
      "            Linear-2            [-1, 256, 1000]       1,001,000\n",
      "            Linear-3            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-4            [-1, 256, 1000]               0\n",
      "         LayerNorm-5            [-1, 256, 1000]           2,000\n",
      "            Linear-6            [-1, 256, 1024]       1,025,024\n",
      "              ReLU-7            [-1, 256, 1024]               0\n",
      "            Linear-8            [-1, 256, 1000]       1,025,000\n",
      "         LayerNorm-9            [-1, 256, 1000]           2,000\n",
      "transformer_block_encoder-10            [-1, 256, 1000]               0\n",
      "           Linear-11            [-1, 256, 1000]       1,001,000\n",
      "           Linear-12            [-1, 256, 1000]       1,001,000\n",
      "           Linear-13            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-14            [-1, 256, 1000]               0\n",
      "        LayerNorm-15            [-1, 256, 1000]           2,000\n",
      "           Linear-16            [-1, 256, 1024]       1,025,024\n",
      "             ReLU-17            [-1, 256, 1024]               0\n",
      "           Linear-18            [-1, 256, 1000]       1,025,000\n",
      "        LayerNorm-19            [-1, 256, 1000]           2,000\n",
      "transformer_block_encoder-20            [-1, 256, 1000]               0\n",
      "================================================================\n",
      "Total params: 10,114,048\n",
      "Trainable params: 10,114,048\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.98\n",
      "Forward/backward pass size (MB): 39.25\n",
      "Params size (MB): 38.58\n",
      "Estimated Total Size (MB): 78.81\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer encoder input size: torch.Size([5, 256, 1000])\n",
      "Transformer encoder output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Cross-attention layer models is: \n",
      "cross_attention_layer(\n",
      "  (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Cross-attention layer input size: torch.Size([5, 256, 1000])\n",
      "Cross-attention layer output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer decoder block models is: \n",
      "transformer_block_decoder(\n",
      "  (attention_): self_attention_layer(\n",
      "    (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  )\n",
      "  (cross_attention_): cross_attention_layer(\n",
      "    (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  )\n",
      "  (layer_norm1_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffnn_): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      "  (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder block models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 256, 1000]       1,001,000\n",
      "            Linear-2            [-1, 256, 1000]       1,001,000\n",
      "            Linear-3            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-4            [-1, 256, 1000]               0\n",
      "         LayerNorm-5            [-1, 256, 1000]           2,000\n",
      "           Dropout-6            [-1, 256, 1000]               0\n",
      "            Linear-7            [-1, 256, 1000]       1,001,000\n",
      "            Linear-8            [-1, 256, 1000]       1,001,000\n",
      "            Linear-9            [-1, 256, 1000]       1,001,000\n",
      "cross_attention_layer-10            [-1, 256, 1000]               0\n",
      "        LayerNorm-11            [-1, 256, 1000]           2,000\n",
      "          Dropout-12            [-1, 256, 1000]               0\n",
      "           Linear-13            [-1, 256, 1024]       1,025,024\n",
      "             ReLU-14            [-1, 256, 1024]               0\n",
      "           Linear-15            [-1, 256, 1000]       1,025,000\n",
      "        LayerNorm-16            [-1, 256, 1000]           2,000\n",
      "          Dropout-17            [-1, 256, 1000]               0\n",
      "================================================================\n",
      "Total params: 8,062,024\n",
      "Trainable params: 8,062,024\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 250000.00\n",
      "Forward/backward pass size (MB): 33.30\n",
      "Params size (MB): 30.75\n",
      "Estimated Total Size (MB): 250064.05\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer block input size: torch.Size([5, 256, 1000])\n",
      "Transformer block output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer decoder models is: \n",
      "transformer_decoder(\n",
      "  (trs_dcdr_blocks_): ModuleList(\n",
      "    (0-1): 2 x transformer_block_decoder(\n",
      "      (attention_): self_attention_layer(\n",
      "        (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "      )\n",
      "      (cross_attention_): cross_attention_layer(\n",
      "        (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "      )\n",
      "      (layer_norm1_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm3_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffnn_): Sequential(\n",
      "        (0): Linear(in_features=1000, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "      )\n",
      "      (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 256, 1000]       1,001,000\n",
      "            Linear-2            [-1, 256, 1000]       1,001,000\n",
      "            Linear-3            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-4            [-1, 256, 1000]               0\n",
      "         LayerNorm-5            [-1, 256, 1000]           2,000\n",
      "           Dropout-6            [-1, 256, 1000]               0\n",
      "            Linear-7            [-1, 256, 1000]       1,001,000\n",
      "            Linear-8            [-1, 256, 1000]       1,001,000\n",
      "            Linear-9            [-1, 256, 1000]       1,001,000\n",
      "cross_attention_layer-10            [-1, 256, 1000]               0\n",
      "        LayerNorm-11            [-1, 256, 1000]           2,000\n",
      "          Dropout-12            [-1, 256, 1000]               0\n",
      "           Linear-13            [-1, 256, 1024]       1,025,024\n",
      "             ReLU-14            [-1, 256, 1024]               0\n",
      "           Linear-15            [-1, 256, 1000]       1,025,000\n",
      "        LayerNorm-16            [-1, 256, 1000]           2,000\n",
      "          Dropout-17            [-1, 256, 1000]               0\n",
      "transformer_block_decoder-18            [-1, 256, 1000]               0\n",
      "           Linear-19            [-1, 256, 1000]       1,001,000\n",
      "           Linear-20            [-1, 256, 1000]       1,001,000\n",
      "           Linear-21            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-22            [-1, 256, 1000]               0\n",
      "        LayerNorm-23            [-1, 256, 1000]           2,000\n",
      "          Dropout-24            [-1, 256, 1000]               0\n",
      "           Linear-25            [-1, 256, 1000]       1,001,000\n",
      "           Linear-26            [-1, 256, 1000]       1,001,000\n",
      "           Linear-27            [-1, 256, 1000]       1,001,000\n",
      "cross_attention_layer-28            [-1, 256, 1000]               0\n",
      "        LayerNorm-29            [-1, 256, 1000]           2,000\n",
      "          Dropout-30            [-1, 256, 1000]               0\n",
      "           Linear-31            [-1, 256, 1024]       1,025,024\n",
      "             ReLU-32            [-1, 256, 1024]               0\n",
      "           Linear-33            [-1, 256, 1000]       1,025,000\n",
      "        LayerNorm-34            [-1, 256, 1000]           2,000\n",
      "          Dropout-35            [-1, 256, 1000]               0\n",
      "transformer_block_decoder-36            [-1, 256, 1000]               0\n",
      "================================================================\n",
      "Total params: 16,124,048\n",
      "Trainable params: 16,124,048\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 250000.00\n",
      "Forward/backward pass size (MB): 70.50\n",
      "Params size (MB): 61.51\n",
      "Estimated Total Size (MB): 250132.01\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder input size: torch.Size([5, 256, 1000])\n",
      "Transformer decoder output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class self_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Self attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Self attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dims_embd_ = dims_embd\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor \n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the self attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        y = []\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        # Compute query, key, value matrices\n",
    "        Q = self.W_q_(x)  # (batch_size, seq_len, dims_embd)\n",
    "        K = self.W_k_(x)  # (batch_size, seq_len, dims_embd)\n",
    "        V = self.W_v_(x)  # (batch_size, seq_len, dims_embd)\n",
    "        \n",
    "        # Compute scaled dot-product attention scores\n",
    "        attention_scores = torch.bmm(Q, K.transpose(1, 2)) / torch.sqrt(torch.tensor(self.dims_embd_, dtype=torch.float32))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute attention output by weighted sum of values\n",
    "        y = torch.bmm(attention_weights, V)  # (batch_size, seq_len, dims_embd)\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    \n",
    "class transformer_block_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        # Self-attention + residual connection\n",
    "        attn_output = self.attention_(x)\n",
    "        x = x + attn_output\n",
    "        x = self.layer_norm1_(x)  # Apply layer normalization\n",
    "        \n",
    "        # Feed-forward network + residual connection\n",
    "        ffnn_output = self.ffnn_(x)\n",
    "        x = x + ffnn_output\n",
    "        x = self.layer_norm2_(x)  # Apply layer normalization\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class transformer_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_encoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer encoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_encoder (int):    Number encoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_endr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_encoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_encoder_ = num_layers_encoder\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        for block in self.trs_endr_blocks_:\n",
    "            x = block(x)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class cross_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Cross attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the cross-attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        # Compute queries, keys, and values\n",
    "        Q = self.W_q_(y)\n",
    "        K = self.W_k_(x)\n",
    "        V = self.W_v_(x)\n",
    "        \n",
    "        # Compute scaled dot-product attention scores\n",
    "        attention_scores = torch.bmm(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.dims_embd_, dtype=torch.float32))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute the weighted sum of values\n",
    "        output = torch.bmm(attention_weights, V)\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    \n",
    "\n",
    "class transformer_block_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single decoder block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        self.cross_attention_ = cross_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm3_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        y = self.layer_norm1_(y + self.attention_(y))\n",
    "        y = self.droput_ops_(y)\n",
    "        y = self.layer_norm2_(y + self.cross_attention_(x, y))\n",
    "        y = self.droput_ops_(y)\n",
    "        y = self.layer_norm3_(y + self.ffnn_(y))\n",
    "        y = self.droput_ops_(y)\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class transformer_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_decoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer decoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_decoder (int):    Number decoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_dcdr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_decoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_decoder_ = num_layers_decoder\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        for block in self.trs_dcdr_blocks_:\n",
    "            x = block(x, y)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "       \n",
    "    \n",
    "dims_embd = 1000\n",
    "num_data_points = 256\n",
    "batch_size = 5\n",
    "num_hidden_nodes_ffnn = 1024\n",
    "dropout_prob = 0.2\n",
    "num_layers_encoder = 2\n",
    "\n",
    "x = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "y = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "\n",
    "# Test Self-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_self_attention_layer = self_attention_layer(dims_embd)\n",
    "print('Self-attention layer models is: \\n{}' .format(model_self_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_self_attention_layer(x)\n",
    "print('Self-attention layer input size: {}' .format(x.shape))\n",
    "print('Self-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "        \n",
    "# Test Transformer encoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_encoder = transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer block models is: \\n{}' .format(model_transformer_block_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_encoder(x)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer encoder input output size \n",
    "print('='*70)\n",
    "model_transformer_encoder = transformer_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer encoder models is: \\n{}' .format(model_transformer_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer encoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_encoder(x)\n",
    "print('Transformer encoder input size: {}' .format(x.shape))\n",
    "print('Transformer encoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# # Test Cross-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_cross_attention_layer = cross_attention_layer(dims_embd)\n",
    "print('Cross-attention layer models is: \\n{}' .format(model_cross_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_cross_attention_layer(x, y)\n",
    "print('Cross-attention layer input size: {}' .format(x.shape))\n",
    "print('Cross-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "\n",
    "# # Test Transformer decoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_decoder = transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer decoder block models is: \\n{}' .format(model_transformer_block_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_decoder(x, y)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# # Test Transformer decoder input output size \n",
    "print('='*70)\n",
    "model_transformer_decoder = transformer_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer decoder models is: \\n{}' .format(model_transformer_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_decoder(x, y)\n",
    "print('Transformer decoder input size: {}' .format(x.shape))\n",
    "print('Transformer decoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe32ebd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ohe\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Example of one-hot encoding a sentence\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m english_encoded \u001b[38;5;241m=\u001b[39m [one_hot_encode(sentence, english_word_to_index) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m english_sentences]\n\u001b[0;32m     29\u001b[0m bengali_encoded \u001b[38;5;241m=\u001b[39m [one_hot_encode(sentence, bengali_word_to_index) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m bengali_sentences]\n",
      "Cell \u001b[1;32mIn[1], line 28\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ohe\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Example of one-hot encoding a sentence\u001b[39;00m\n\u001b[1;32m---> 28\u001b[0m english_encoded \u001b[38;5;241m=\u001b[39m [\u001b[43mone_hot_encode\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menglish_word_to_index\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m english_sentences]\n\u001b[0;32m     29\u001b[0m bengali_encoded \u001b[38;5;241m=\u001b[39m [one_hot_encode(sentence, bengali_word_to_index) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m bengali_sentences]\n",
      "Cell \u001b[1;32mIn[1], line 23\u001b[0m, in \u001b[0;36mone_hot_encode\u001b[1;34m(sentence, word_to_index)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mone_hot_encode\u001b[39m(sentence, word_to_index):\n\u001b[0;32m     22\u001b[0m     indices \u001b[38;5;241m=\u001b[39m [word_to_index[word] \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m sentence\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()]\n\u001b[1;32m---> 23\u001b[0m     ohe \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mzeros((\u001b[38;5;28mlen\u001b[39m(indices), \u001b[38;5;28mlen\u001b[39m(word_to_index)))\n\u001b[0;32m     24\u001b[0m     ohe[\u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(indices)), indices] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ohe\n",
      "\u001b[1;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 1: Load the data\n",
    "def load_data(english_file, bengali_file):\n",
    "    with open(english_file, 'r', encoding='utf-8') as f:\n",
    "        english_sentences = f.readlines()\n",
    "    with open(bengali_file, 'r', encoding='utf-8') as f:\n",
    "        bengali_sentences = f.readlines()\n",
    "    return english_sentences, bengali_sentences\n",
    "\n",
    "english_sentences, bengali_sentences = load_data('OpenSubtitles.bn-en.en', 'OpenSubtitles.bn-en.bn')\n",
    "\n",
    "# Step 2: Tokenize sentences and create vocabularies\n",
    "def tokenize_and_build_vocab(sentences):\n",
    "    vocab = set(word for sentence in sentences for word in sentence.strip().split())\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return word_to_index, vocab\n",
    "\n",
    "english_word_to_index, english_vocab = tokenize_and_build_vocab(english_sentences)\n",
    "bengali_word_to_index, bengali_vocab = tokenize_and_build_vocab(bengali_sentences)\n",
    "\n",
    "# Step 3: One-Hot Encoding using Sklearn OneHotEncoder\n",
    "def one_hot_encode(sentence, word_to_index):\n",
    "    indices = [word_to_index[word] for word in sentence.strip().split()]\n",
    "    ohe = torch.zeros((len(indices), len(word_to_index)))\n",
    "    ohe[range(len(indices)), indices] = 1.0\n",
    "    return ohe\n",
    "\n",
    "# Example of one-hot encoding a sentence\n",
    "english_encoded = [one_hot_encode(sentence, english_word_to_index) for sentence in english_sentences]\n",
    "bengali_encoded = [one_hot_encode(sentence, bengali_word_to_index) for sentence in bengali_sentences]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
