{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "62f8a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------\n",
    "Transformer using pytorch and numpy\n",
    "-----------------------------------------------------------------------------\n",
    "AUTHOR: Soumitra Samanta (soumitra.samanta@gm.rkmvu.ac.in)\n",
    "-----------------------------------------------------------------------------\n",
    "Package required:\n",
    "Numpy: https://numpy.org/\n",
    "Matplotlib: https://matplotlib.org\n",
    "-----------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "from typing import Tuple\n",
    "\n",
    "\n",
    "# import torch\n",
    "import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d4aa92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Self-attention layer models is: \n",
      "self_attention_layer(\n",
      "  (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Self-attention layer input size: torch.Size([5, 256, 1000])\n",
      "Self-attention layer output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer block models is: \n",
      "transformer_block_encoder(\n",
      "  (attention_): self_attention_layer(\n",
      "    (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  )\n",
      "  (layer_norm1_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffnn_): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      "  (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer block models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 256, 1000]       1,001,000\n",
      "            Linear-2            [-1, 256, 1000]       1,001,000\n",
      "            Linear-3            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-4            [-1, 256, 1000]               0\n",
      "         LayerNorm-5            [-1, 256, 1000]           2,000\n",
      "            Linear-6            [-1, 256, 1024]       1,025,024\n",
      "              ReLU-7            [-1, 256, 1024]               0\n",
      "            Linear-8            [-1, 256, 1000]       1,025,000\n",
      "         LayerNorm-9            [-1, 256, 1000]           2,000\n",
      "================================================================\n",
      "Total params: 5,057,024\n",
      "Trainable params: 5,057,024\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.98\n",
      "Forward/backward pass size (MB): 17.67\n",
      "Params size (MB): 19.29\n",
      "Estimated Total Size (MB): 37.94\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer block input size: torch.Size([5, 256, 1000])\n",
      "Transformer block output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer encoder models is: \n",
      "transformer_encoder(\n",
      "  (trs_endr_blocks_): ModuleList(\n",
      "    (0-1): 2 x transformer_block_encoder(\n",
      "      (attention_): self_attention_layer(\n",
      "        (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "      )\n",
      "      (layer_norm1_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffnn_): Sequential(\n",
      "        (0): Linear(in_features=1000, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "      )\n",
      "      (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer encoder models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 256, 1000]       1,001,000\n",
      "            Linear-2            [-1, 256, 1000]       1,001,000\n",
      "            Linear-3            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-4            [-1, 256, 1000]               0\n",
      "         LayerNorm-5            [-1, 256, 1000]           2,000\n",
      "            Linear-6            [-1, 256, 1024]       1,025,024\n",
      "              ReLU-7            [-1, 256, 1024]               0\n",
      "            Linear-8            [-1, 256, 1000]       1,025,000\n",
      "         LayerNorm-9            [-1, 256, 1000]           2,000\n",
      "transformer_block_encoder-10            [-1, 256, 1000]               0\n",
      "           Linear-11            [-1, 256, 1000]       1,001,000\n",
      "           Linear-12            [-1, 256, 1000]       1,001,000\n",
      "           Linear-13            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-14            [-1, 256, 1000]               0\n",
      "        LayerNorm-15            [-1, 256, 1000]           2,000\n",
      "           Linear-16            [-1, 256, 1024]       1,025,024\n",
      "             ReLU-17            [-1, 256, 1024]               0\n",
      "           Linear-18            [-1, 256, 1000]       1,025,000\n",
      "        LayerNorm-19            [-1, 256, 1000]           2,000\n",
      "transformer_block_encoder-20            [-1, 256, 1000]               0\n",
      "================================================================\n",
      "Total params: 10,114,048\n",
      "Trainable params: 10,114,048\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.98\n",
      "Forward/backward pass size (MB): 39.25\n",
      "Params size (MB): 38.58\n",
      "Estimated Total Size (MB): 78.81\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer encoder input size: torch.Size([5, 256, 1000])\n",
      "Transformer encoder output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Cross-attention layer models is: \n",
      "cross_attention_layer(\n",
      "  (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Cross-attention layer input size: torch.Size([5, 256, 1000])\n",
      "Cross-attention layer output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer decoder block models is: \n",
      "transformer_block_decoder(\n",
      "  (attention_): self_attention_layer(\n",
      "    (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  )\n",
      "  (cross_attention_): cross_attention_layer(\n",
      "    (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "    (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "  )\n",
      "  (layer_norm1_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm2_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "  (layer_norm3_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "  (ffnn_): Sequential(\n",
      "    (0): Linear(in_features=1000, out_features=1024, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "  )\n",
      "  (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder block models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 256, 1000]       1,001,000\n",
      "            Linear-2            [-1, 256, 1000]       1,001,000\n",
      "            Linear-3            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-4            [-1, 256, 1000]               0\n",
      "         LayerNorm-5            [-1, 256, 1000]           2,000\n",
      "           Dropout-6            [-1, 256, 1000]               0\n",
      "            Linear-7            [-1, 256, 1000]       1,001,000\n",
      "            Linear-8            [-1, 256, 1000]       1,001,000\n",
      "            Linear-9            [-1, 256, 1000]       1,001,000\n",
      "cross_attention_layer-10            [-1, 256, 1000]               0\n",
      "        LayerNorm-11            [-1, 256, 1000]           2,000\n",
      "          Dropout-12            [-1, 256, 1000]               0\n",
      "           Linear-13            [-1, 256, 1024]       1,025,024\n",
      "             ReLU-14            [-1, 256, 1024]               0\n",
      "           Linear-15            [-1, 256, 1000]       1,025,000\n",
      "        LayerNorm-16            [-1, 256, 1000]           2,000\n",
      "          Dropout-17            [-1, 256, 1000]               0\n",
      "================================================================\n",
      "Total params: 8,062,024\n",
      "Trainable params: 8,062,024\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 250000.00\n",
      "Forward/backward pass size (MB): 33.30\n",
      "Params size (MB): 30.75\n",
      "Estimated Total Size (MB): 250064.05\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer block input size: torch.Size([5, 256, 1000])\n",
      "Transformer block output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n",
      "======================================================================\n",
      "Transformer decoder models is: \n",
      "transformer_decoder(\n",
      "  (trs_dcdr_blocks_): ModuleList(\n",
      "    (0-1): 2 x transformer_block_decoder(\n",
      "      (attention_): self_attention_layer(\n",
      "        (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "      )\n",
      "      (cross_attention_): cross_attention_layer(\n",
      "        (W_q_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_k_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "        (W_v_): Linear(in_features=1000, out_features=1000, bias=True)\n",
      "      )\n",
      "      (layer_norm1_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm2_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "      (layer_norm3_): LayerNorm((1000,), eps=1e-05, elementwise_affine=True)\n",
      "      (ffnn_): Sequential(\n",
      "        (0): Linear(in_features=1000, out_features=1024, bias=True)\n",
      "        (1): ReLU()\n",
      "        (2): Linear(in_features=1024, out_features=1000, bias=True)\n",
      "      )\n",
      "      (droput_ops_): Dropout(p=0.2, inplace=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder models summary:\n",
      "----------------------------------------------------------------------\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Linear-1            [-1, 256, 1000]       1,001,000\n",
      "            Linear-2            [-1, 256, 1000]       1,001,000\n",
      "            Linear-3            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-4            [-1, 256, 1000]               0\n",
      "         LayerNorm-5            [-1, 256, 1000]           2,000\n",
      "           Dropout-6            [-1, 256, 1000]               0\n",
      "            Linear-7            [-1, 256, 1000]       1,001,000\n",
      "            Linear-8            [-1, 256, 1000]       1,001,000\n",
      "            Linear-9            [-1, 256, 1000]       1,001,000\n",
      "cross_attention_layer-10            [-1, 256, 1000]               0\n",
      "        LayerNorm-11            [-1, 256, 1000]           2,000\n",
      "          Dropout-12            [-1, 256, 1000]               0\n",
      "           Linear-13            [-1, 256, 1024]       1,025,024\n",
      "             ReLU-14            [-1, 256, 1024]               0\n",
      "           Linear-15            [-1, 256, 1000]       1,025,000\n",
      "        LayerNorm-16            [-1, 256, 1000]           2,000\n",
      "          Dropout-17            [-1, 256, 1000]               0\n",
      "transformer_block_decoder-18            [-1, 256, 1000]               0\n",
      "           Linear-19            [-1, 256, 1000]       1,001,000\n",
      "           Linear-20            [-1, 256, 1000]       1,001,000\n",
      "           Linear-21            [-1, 256, 1000]       1,001,000\n",
      "self_attention_layer-22            [-1, 256, 1000]               0\n",
      "        LayerNorm-23            [-1, 256, 1000]           2,000\n",
      "          Dropout-24            [-1, 256, 1000]               0\n",
      "           Linear-25            [-1, 256, 1000]       1,001,000\n",
      "           Linear-26            [-1, 256, 1000]       1,001,000\n",
      "           Linear-27            [-1, 256, 1000]       1,001,000\n",
      "cross_attention_layer-28            [-1, 256, 1000]               0\n",
      "        LayerNorm-29            [-1, 256, 1000]           2,000\n",
      "          Dropout-30            [-1, 256, 1000]               0\n",
      "           Linear-31            [-1, 256, 1024]       1,025,024\n",
      "             ReLU-32            [-1, 256, 1024]               0\n",
      "           Linear-33            [-1, 256, 1000]       1,025,000\n",
      "        LayerNorm-34            [-1, 256, 1000]           2,000\n",
      "          Dropout-35            [-1, 256, 1000]               0\n",
      "transformer_block_decoder-36            [-1, 256, 1000]               0\n",
      "================================================================\n",
      "Total params: 16,124,048\n",
      "Trainable params: 16,124,048\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 250000.00\n",
      "Forward/backward pass size (MB): 70.50\n",
      "Params size (MB): 61.51\n",
      "Estimated Total Size (MB): 250132.01\n",
      "----------------------------------------------------------------\n",
      "----------------------------------------------------------------------\n",
      "Transformer decoder input size: torch.Size([5, 256, 1000])\n",
      "Transformer decoder output size: torch.Size([5, 256, 1000])\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class self_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Self attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Self attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dims_embd_ = dims_embd\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor \n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the self attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        y = []\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        # Compute query, key, value matrices\n",
    "        Q = self.W_q_(x)  # (batch_size, seq_len, dims_embd)\n",
    "        K = self.W_k_(x)  # (batch_size, seq_len, dims_embd)\n",
    "        V = self.W_v_(x)  # (batch_size, seq_len, dims_embd)\n",
    "        \n",
    "        # Compute scaled dot-product attention scores\n",
    "        attention_scores = torch.bmm(Q, K.transpose(1, 2)) / torch.sqrt(torch.tensor(self.dims_embd_, dtype=torch.float32))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute attention output by weighted sum of values\n",
    "        y = torch.bmm(attention_weights, V)  # (batch_size, seq_len, dims_embd)\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    \n",
    "class transformer_block_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        # Self-attention + residual connection\n",
    "        attn_output = self.attention_(x)\n",
    "        x = x + attn_output\n",
    "        x = self.layer_norm1_(x)  # Apply layer normalization\n",
    "        \n",
    "        # Feed-forward network + residual connection\n",
    "        ffnn_output = self.ffnn_(x)\n",
    "        x = x + ffnn_output\n",
    "        x = self.layer_norm2_(x)  # Apply layer normalization\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class transformer_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_encoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer encoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_encoder (int):    Number encoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_endr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_encoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_encoder_ = num_layers_encoder\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        for block in self.trs_endr_blocks_:\n",
    "            x = block(x)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class cross_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Cross attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the cross-attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        # Compute queries, keys, and values\n",
    "        Q = self.W_q_(y)\n",
    "        K = self.W_k_(x)\n",
    "        V = self.W_v_(x)\n",
    "        \n",
    "        # Compute scaled dot-product attention scores\n",
    "        attention_scores = torch.bmm(Q, K.transpose(-2, -1)) / torch.sqrt(torch.tensor(self.dims_embd_, dtype=torch.float32))\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = torch.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute the weighted sum of values\n",
    "        output = torch.bmm(attention_weights, V)\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    \n",
    "\n",
    "class transformer_block_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single decoder block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        self.cross_attention_ = cross_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm3_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        y = self.layer_norm1_(y + self.attention_(y))\n",
    "        y = self.droput_ops_(y)\n",
    "        y = self.layer_norm2_(y + self.cross_attention_(x, y))\n",
    "        y = self.droput_ops_(y)\n",
    "        y = self.layer_norm3_(y + self.ffnn_(y))\n",
    "        y = self.droput_ops_(y)\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class transformer_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_decoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer decoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_decoder (int):    Number decoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_dcdr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_decoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_decoder_ = num_layers_decoder\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        for block in self.trs_dcdr_blocks_:\n",
    "            x = block(x, y)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "       \n",
    "    \n",
    "dims_embd = 1000\n",
    "num_data_points = 256\n",
    "batch_size = 5\n",
    "num_hidden_nodes_ffnn = 1024\n",
    "dropout_prob = 0.2\n",
    "num_layers_encoder = 2\n",
    "\n",
    "x = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "y = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "\n",
    "# Test Self-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_self_attention_layer = self_attention_layer(dims_embd)\n",
    "print('Self-attention layer models is: \\n{}' .format(model_self_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_self_attention_layer(x)\n",
    "print('Self-attention layer input size: {}' .format(x.shape))\n",
    "print('Self-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "        \n",
    "# Test Transformer encoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_encoder = transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer block models is: \\n{}' .format(model_transformer_block_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_encoder(x)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# Test Transformer encoder input output size \n",
    "print('='*70)\n",
    "model_transformer_encoder = transformer_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer encoder models is: \\n{}' .format(model_transformer_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer encoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_encoder(x)\n",
    "print('Transformer encoder input size: {}' .format(x.shape))\n",
    "print('Transformer encoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# # Test Cross-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_cross_attention_layer = cross_attention_layer(dims_embd)\n",
    "print('Cross-attention layer models is: \\n{}' .format(model_cross_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_cross_attention_layer(x, y)\n",
    "print('Cross-attention layer input size: {}' .format(x.shape))\n",
    "print('Cross-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "\n",
    "# # Test Transformer decoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_decoder = transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer decoder block models is: \\n{}' .format(model_transformer_block_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_block_decoder(x, y)\n",
    "print('Transformer block input size: {}' .format(x.shape))\n",
    "print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "\n",
    "# # Test Transformer decoder input output size \n",
    "print('='*70)\n",
    "model_transformer_decoder = transformer_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "print('Transformer decoder models is: \\n{}' .format(model_transformer_decoder))\n",
    "print('-'*70)\n",
    "print('Transformer decoder models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_transformer_decoder(x, y)\n",
    "print('Transformer decoder input size: {}' .format(x.shape))\n",
    "print('Transformer decoder output size: {}' .format(y_bar.shape))  \n",
    "print('-'*70)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fe32ebd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load the data\n",
    "def load_data(english_file, bengali_file):\n",
    "    with open(english_file, 'r', encoding='utf-8') as f:\n",
    "        english_sentences = f.readlines()\n",
    "    with open(bengali_file, 'r', encoding='utf-8') as f:\n",
    "        bengali_sentences = f.readlines()\n",
    "    return english_sentences, bengali_sentences\n",
    "\n",
    "english_sentences, bengali_sentences = load_data('OpenSubtitles.bn-en.en', 'OpenSubtitles.bn-en.bn')\n",
    "\n",
    "# Step 2: Tokenize sentences and create vocabularies\n",
    "def tokenize_and_build_vocab(sentences):\n",
    "    vocab = set(word for sentence in sentences for word in sentence.strip().split())\n",
    "    word_to_index = {word: idx for idx, word in enumerate(vocab)}\n",
    "    return word_to_index, vocab\n",
    "\n",
    "english_word_to_index, english_vocab = tokenize_and_build_vocab(english_sentences)\n",
    "bengali_word_to_index, bengali_vocab = tokenize_and_build_vocab(bengali_sentences)\n",
    "\n",
    "# Step 3: One-Hot Encoding using Sklearn OneHotEncoder\n",
    "def one_hot_encode(sentence, word_to_index):\n",
    "    indices = [word_to_index[word] for word in sentence.strip().split()]\n",
    "    ohe = torch.zeros((len(indices), len(word_to_index)))\n",
    "    ohe[range(len(indices)), indices] = 1.0\n",
    "    return ohe\n",
    "\n",
    "# Example of one-hot encoding a sentence\n",
    "english_encoded = [one_hot_encode(sentence, english_word_to_index) for sentence in english_sentences]\n",
    "bengali_encoded = [one_hot_encode(sentence, bengali_word_to_index) for sentence in bengali_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94f55d5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (126) must match the size of tensor b (124) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 42\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Apply positional encoding to each one-hot encoded embedding\u001b[39;00m\n\u001b[1;32m     41\u001b[0m english_pos_encoded \u001b[38;5;241m=\u001b[39m [pos_encoding(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m english_encoded]\n\u001b[0;32m---> 42\u001b[0m bengali_pos_encoded \u001b[38;5;241m=\u001b[39m [pos_encoding(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m bengali_encoded]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Check the shape to ensure it matches\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(english_pos_encoded[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should show (sequence_length, embedding_dim) for each sentence\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 42\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m# Apply positional encoding to each one-hot encoded embedding\u001b[39;00m\n\u001b[1;32m     41\u001b[0m english_pos_encoded \u001b[38;5;241m=\u001b[39m [pos_encoding(sentence) \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m english_encoded]\n\u001b[0;32m---> 42\u001b[0m bengali_pos_encoded \u001b[38;5;241m=\u001b[39m [\u001b[43mpos_encoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msentence\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m bengali_encoded]\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m# Check the shape to ensure it matches\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(english_pos_encoded[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# Should show (sequence_length, embedding_dim) for each sentence\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[11], line 32\u001b[0m, in \u001b[0;36mPositionalEncoding.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     29\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x, padding], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# Add positional encoding to embeddings\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mpe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (126) must match the size of tensor b (124) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# Define the PositionalEncoding class\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "    def __init__(self, embedding_dim, max_len=128):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        \n",
    "        # Initialize the positional encoding matrix\n",
    "        pe = torch.zeros(max_len, embedding_dim)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embedding_dim, 2).float() * (-math.log(10000.0) / embedding_dim))\n",
    "        \n",
    "        # Apply sine to even indices and cosine to odd indices\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # Add batch dimension\n",
    "\n",
    "    def forward(self, x):\n",
    "        seq_len = x.size(0)\n",
    "        \n",
    "        # Ensure positional encoding tensor matches sentence length\n",
    "        pe = self.pe[:, :seq_len, :].to(x.device)\n",
    "        \n",
    "        # Expand `x` to embedding dimension if needed\n",
    "        if x.size(1) < self.embedding_dim:\n",
    "            padding = torch.zeros(seq_len, self.embedding_dim - x.size(1)).to(x.device)\n",
    "            x = torch.cat([x, padding], dim=1)\n",
    "        \n",
    "        # Add positional encoding to embeddings\n",
    "        x = x + pe.squeeze(0)\n",
    "        return x\n",
    "\n",
    "# Instantiate positional encoding with embedding dimension equal to vocab size\n",
    "embedding_dim = len(english_word_to_index)  # This matches the number of one-hot columns\n",
    "max_len = 128  # Set max length according to the max sentence length in your data\n",
    "pos_encoding = PositionalEncoding(embedding_dim, max_len=max_len)\n",
    "\n",
    "# Apply positional encoding to each one-hot encoded embedding\n",
    "english_pos_encoded = [pos_encoding(sentence) for sentence in english_encoded]\n",
    "bengali_pos_encoded = [pos_encoding(sentence) for sentence in bengali_encoded]\n",
    "\n",
    "# Check the shape to ensure it matches\n",
    "print(english_pos_encoded[0].shape)  # Should show (sequence_length, embedding_dim) for each sentence\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
