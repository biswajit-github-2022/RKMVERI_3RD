{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62f8a9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "-----------------------------------------------------------------------------\n",
    "Transformer using pytorch and numpy\n",
    "-----------------------------------------------------------------------------\n",
    "AUTHOR: Soumitra Samanta (soumitra.samanta@gm.rkmvu.ac.in)\n",
    "-----------------------------------------------------------------------------\n",
    "Package required:\n",
    "Numpy: https://numpy.org/\n",
    "Matplotlib: https://matplotlib.org\n",
    "-----------------------------------------------------------------------------\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchsummary import summary\n",
    "\n",
    "from typing import Tuple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d4aa92e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "Self-attention layer models is: \n",
      "self_attention_layer(\n",
      "  (W_q_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_k_): Linear(in_features=10, out_features=10, bias=True)\n",
      "  (W_v_): Linear(in_features=10, out_features=10, bias=True)\n",
      ")\n",
      "----------------------------------------------------------------------\n",
      "Self-attention layer input size: torch.Size([5, 100, 10])\n",
      "Self-attention layer output size: torch.Size([5, 100, 10])\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "class self_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Self attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Self attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        self.dims_embd_ = dims_embd\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor \n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the self attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        y = []\n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        # Compute query, key, value matrices\n",
    "        Q = self.W_q_(x)  # (batch_size, seq_len, dims_embd)\n",
    "        K = self.W_k_(x)  # (batch_size, seq_len, dims_embd)\n",
    "        V = self.W_v_(x)  # (batch_size, seq_len, dims_embd)\n",
    "        \n",
    "        # Compute scaled dot-product attention scores\n",
    "        attention_scores = torch.bmm(Q, K.transpose(1, 2)) / torch.sqrt(torch.tensor(self.dims_embd_, dtype=torch.float32))\n",
    "\n",
    "        # Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)\n",
    "        \n",
    "        # Compute attention output by weighted sum of values\n",
    "        y = torch.bmm(attention_weights, V)  # (batch_size, seq_len, dims_embd)\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    \n",
    "class transformer_block_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        # Self-attention + residual connection\n",
    "        attn_output = self.attention_(x)\n",
    "        x = x + attn_output\n",
    "        x = self.layer_norm1_(x)  # Apply layer normalization\n",
    "        \n",
    "        # Feed-forward network + residual connection\n",
    "        ffnn_output = self.ffnn_(x)\n",
    "        x = x + ffnn_output\n",
    "        x = self.layer_norm2_(x)  # Apply layer normalization\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "class transformer_encoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer encoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_encoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer encoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_encoder (int):    Number encoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_endr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_encoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_encoder_ = num_layers_encoder\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class cross_attention_layer(nn.Module):\n",
    "    \"\"\"\n",
    "    Cross attention layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Cross attention class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int): Embedding dimension\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.W_q_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_k_ = nn.Linear(dims_embd, dims_embd)\n",
    "        self.W_v_ = nn.Linear(dims_embd, dims_embd)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self, \n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the cross-attention layer\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        \n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "    \n",
    "        return y\n",
    "    \n",
    "\n",
    "class transformer_block_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer single decoder block\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer single block class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "        \"\"\"\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        self.attention_ = self_attention_layer(dims_embd)\n",
    "        self.cross_attention_ = cross_attention_layer(dims_embd)\n",
    "        \n",
    "        self.layer_norm1_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm2_ = nn.LayerNorm(dims_embd)\n",
    "        self.layer_norm3_ = nn.LayerNorm(dims_embd)\n",
    "        \n",
    "        self.ffnn_ = nn.Sequential(\n",
    "            nn.Linear(dims_embd, num_hidden_nodes_ffnn),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(num_hidden_nodes_ffnn, dims_embd)\n",
    "        )\n",
    "        self.droput_ops_ = nn.Dropout(dropout_prob)\n",
    "        \n",
    "        self.dims_embd_ = dims_embd\n",
    "        self.num_hidden_nodes_ffnn_ = num_hidden_nodes_ffnn\n",
    "        self.dropout_prob_ = dropout_prob\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer block\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        x = self.layer_norm1_(x + self.attention_(x))\n",
    "        x = self.droput_ops_(x)\n",
    "        x = self.layer_norm2_(x + self.cross_attention_(x, y))\n",
    "        x = self.droput_ops_(x)\n",
    "        x = self.layer_norm3_(x + self.ffnn_(x))\n",
    "        x = self.droput_ops_(x)\n",
    "    \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    \n",
    "class transformer_decoder(nn.Module):\n",
    "    \"\"\"\n",
    "    Transformer decoder module\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        dims_embd: int,\n",
    "        num_hidden_nodes_ffnn: int = 2048,\n",
    "        dropout_prob: float = 0.0,\n",
    "        num_layers_decoder: int = 2\n",
    "    )->None:\n",
    "        \"\"\"\n",
    "        Transformer decoder class initialization\n",
    "        \n",
    "        Inpout:\n",
    "            - dims_embd (int):             Embedding dimension\n",
    "            - num_hidden_nodes_ffnn (int): Number of neurons in the fed-forward layer\n",
    "            - dropout_prob (float):        Dropout probability in liner layers\n",
    "            - num_layers_decoder (int):    Number decoder blocks\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        self.trs_dcdr_blocks_ = nn.ModuleList(\n",
    "            [\n",
    "                transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob) for _ in range(num_layers_decoder)\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        self.num_layers_decoder_ = num_layers_decoder\n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        x: Tensor,\n",
    "        y: Tensor\n",
    "    )->Tensor:\n",
    "        \"\"\"\n",
    "        Forward pass for the transformer encoder\n",
    "        \n",
    "        Imput:\n",
    "            - x (torch tensor): Input encoder data\n",
    "            - y (torch tensor): Input decoder data\n",
    "            \n",
    "        Output:\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        ############################################################################\n",
    "        #                             Your code will be here                       #\n",
    "        #--------------------------------------------------------------------------#\n",
    "        \n",
    "        for block in self.trs_dcdr_blocks_:\n",
    "            x = block(x, y)\n",
    "        \n",
    "        #--------------------------------------------------------------------------#\n",
    "        #                             End of your code                             #\n",
    "        ############################################################################\n",
    "        \n",
    "        return x\n",
    "        \n",
    "       \n",
    "    \n",
    "dims_embd = 10\n",
    "num_data_points = 100\n",
    "batch_size = 5\n",
    "num_hidden_nodes_ffnn = 1024\n",
    "dropout_prob = 0.2\n",
    "num_layers_encoder = 2\n",
    "\n",
    "x = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "y = torch.rand(batch_size, num_data_points, dims_embd)\n",
    "\n",
    "# Test Self-attention layer and its input output size  \n",
    "print('='*70)\n",
    "model_self_attention_layer = self_attention_layer(dims_embd)\n",
    "print('Self-attention layer models is: \\n{}' .format(model_self_attention_layer))\n",
    "print('-'*70)\n",
    "\n",
    "y_bar = model_self_attention_layer(x)\n",
    "print('Self-attention layer input size: {}' .format(x.shape))\n",
    "print('Self-attention layer output size: {}' .format(y_bar.shape))\n",
    "print('-'*70)\n",
    "        \n",
    "# Test Transformer encoder block input output size \n",
    "print('='*70)\n",
    "model_transformer_block_encoder = transformer_block_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "print('Transformer block models is: \\n{}' .format(model_transformer_block_encoder))\n",
    "print('-'*70)\n",
    "print('Transformer block models summary:')\n",
    "print('-'*70)\n",
    "summary(model_transformer_block_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "print('-'*70)\n",
    "\n",
    "# y_bar = model_transformer_block_encoder(x)\n",
    "# print('Transformer block input size: {}' .format(x.shape))\n",
    "# print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "# print('-'*70)\n",
    "\n",
    "# # Test Transformer encoder input output size \n",
    "# print('='*70)\n",
    "# model_transformer_encoder = transformer_encoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "# print('Transformer encoder models is: \\n{}' .format(model_transformer_encoder))\n",
    "# print('-'*70)\n",
    "# print('Transformer encoder models summary:')\n",
    "# print('-'*70)\n",
    "# summary(model_transformer_encoder, (num_data_points, dims_embd, ), device=str(\"cpu\"))\n",
    "# print('-'*70)\n",
    "\n",
    "# y_bar = model_transformer_encoder(x)\n",
    "# print('Transformer encoder input size: {}' .format(x.shape))\n",
    "# print('Transformer encoder output size: {}' .format(y_bar.shape))  \n",
    "# print('-'*70)\n",
    "\n",
    "# # Test Cross-attention layer and its input output size  \n",
    "# print('='*70)\n",
    "# model_cross_attention_layer = cross_attention_layer(dims_embd)\n",
    "# print('Cross-attention layer models is: \\n{}' .format(model_cross_attention_layer))\n",
    "# print('-'*70)\n",
    "\n",
    "# y_bar = model_cross_attention_layer(x, y)\n",
    "# print('Cross-attention layer input size: {}' .format(x.shape))\n",
    "# print('Cross-attention layer output size: {}' .format(y_bar.shape))\n",
    "# print('-'*70)\n",
    "\n",
    "# # Test Transformer decoder block input output size \n",
    "# print('='*70)\n",
    "# model_transformer_block_decoder = transformer_block_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob)\n",
    "# print('Transformer decoder block models is: \\n{}' .format(model_transformer_block_decoder))\n",
    "# print('-'*70)\n",
    "# print('Transformer decoder block models summary:')\n",
    "# print('-'*70)\n",
    "# summary(model_transformer_block_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "# print('-'*70)\n",
    "\n",
    "# y_bar = model_transformer_block_decoder(x, y)\n",
    "# print('Transformer block input size: {}' .format(x.shape))\n",
    "# print('Transformer block output size: {}' .format(y_bar.shape))  \n",
    "# print('-'*70)\n",
    "\n",
    "# # Test Transformer decoder input output size \n",
    "# print('='*70)\n",
    "# model_transformer_decoder = transformer_decoder(dims_embd, num_hidden_nodes_ffnn, dropout_prob, num_layers_encoder)\n",
    "# print('Transformer decoder models is: \\n{}' .format(model_transformer_decoder))\n",
    "# print('-'*70)\n",
    "# print('Transformer decoder models summary:')\n",
    "# print('-'*70)\n",
    "# summary(model_transformer_decoder, [(num_data_points, dims_embd, ), (num_data_points, dims_embd, )], device=str(\"cpu\"))\n",
    "# print('-'*70)\n",
    "\n",
    "# y_bar = model_transformer_decoder(x, y)\n",
    "# print('Transformer decoder input size: {}' .format(x.shape))\n",
    "# print('Transformer decoder output size: {}' .format(y_bar.shape))  \n",
    "# print('-'*70)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db4fe3ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
