{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAVQAAADnCAYAAABBu67aAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAFMklEQVR4nO3cvW4cZRiG4XeTTUAkyIqIgoGCBkKTnoQKyUeAXFBSUXAGnATHQOWCwpJ7UxiJggNwQZEoUQCTP4xN/Lf2rocKhIRnZ1Z5CHF8XeV+r2anujU7M/sNmqYpAJ7duf/7BABeFoIKECKoACGCChAiqAAhw451rwAA/NvgpA9doQKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqQIigAoQIKkCIoAKECCpAiKAChAgqwAyapmldE1SAGexv/tK6JqgAMzj4faN1TVABemqapnYf32tdF1SAvprj2n10r3VZUAF6Go9263Bns3VdUAF6OtrdrvHBTuu6oAL0tPvobjWTo9Z1QQXooWma2nl4Z+qMoAL00EyOpr6DWiWoAL2MR7t1tLc9dUZQAXo4fLo59YFUlaACdPr7hf7meOqcoAL0sPfkfueMoAJ0aI7HNdp+2DknqAAdJqO9Gv3xuHNOUAE6HGw9qPFor3NOUAE67P32c+cDqaqq4XM4F4AXyrRd908Yrp0Ht3uNCipw5iwtLdXKykqv2YvDc/XZh3N19VL3D3pBBc6c9fX1Wl5e7jX7ztXX6/Nbn9aT0dt13JyvKxfbn/YLKsAU7771Zt3e/7h+Hb1fTQ3q8vntutkyK6gAUwznbtXG6IOqGlRV1c7kSuusp/wAU8zPX6+/YtpFUAFavHpxWB+9d1CDmvSaF1SAFheG5+qnu9/VYPvb2t/frub4oK690v6f/qn3UCeTflUGOE36vof6dO+wvvrm+7ow/KEuXb5W82/M1c3rr9UXX35y4vzUoK6trc18ogAvuvv3u3eO+qej8bi2tjZqa2ujfrxT9XXL3NSgLiwszPSlAKfB6urqf3Jc91ABQgQVIERQAUIEFSBEUAFCBBUgxOYowJlz48aNWlxcjB930PGPgRm2tQY4HWbasf8Eg8HgxN1SXKECZ05LD5+Ze6gAIYIKECKoACGCChAiqAAhggoQIqgAIYIKECKoACGCChAiqAAhggoQIqgAIYIKECKoACGCChAiqAAhggoQIqgAIYIKECKoACGCChAiqAAhggoQIqgAIYIKECKoACGCChAiqAAhggoQIqgAIYIKECKoACGCChAiqAAhggoQIqgAIYIKECKoACGCChAiqAAhggoQIqgAIYIKEDLsWB88l7MAeAm4QgUIEVSAEEEFCBFUgBBBBQgRVICQPwFQYLqQW9coMAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 finished with total reward: 139.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Define the Q-Network (a simple feedforward neural network)\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "# Function to select action based on epsilon-greedy policy\n",
    "def select_action(state, policy_net, epsilon, action_dim):\n",
    "    if random.random() > epsilon:  # Exploit\n",
    "        with torch.no_grad():\n",
    "            return policy_net(state).argmax().item()  # Choose action with highest Q-value\n",
    "    else:  # Explore\n",
    "        return random.choice(np.arange(action_dim))\n",
    "\n",
    "# Function to optimize the model\n",
    "def optimize_model(policy_net, target_net, optimizer, memory, batch_size, gamma):\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "\n",
    "    # Sample a random minibatch of transitions from memory\n",
    "    batch = random.sample(memory, batch_size)\n",
    "    batch = list(zip(*batch))\n",
    "\n",
    "    states = torch.tensor(np.vstack(batch[0]), dtype=torch.float32)\n",
    "    actions = torch.tensor(batch[1], dtype=torch.long)\n",
    "    rewards = torch.tensor(batch[2], dtype=torch.float32)\n",
    "    next_states = torch.tensor(np.vstack(batch[3]), dtype=torch.float32)\n",
    "    dones = torch.tensor(batch[4], dtype=torch.float32)\n",
    "\n",
    "    # Compute Q-values for current states (policy network)\n",
    "    q_values = policy_net(states).gather(1, actions.view(-1, 1)).squeeze()\n",
    "\n",
    "    # Compute Q-values for next states (target network)\n",
    "    next_q_values = target_net(next_states).max(1)[0]\n",
    "\n",
    "    # Compute target Q-values\n",
    "    target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n",
    "\n",
    "    # Compute loss (Mean Squared Error)\n",
    "    loss = nn.MSELoss()(q_values, target_q_values)\n",
    "\n",
    "    # Perform gradient descent\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# DQN training function\n",
    "def train_dqn(env, policy_net, target_net, optimizer, num_episodes, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay, target_update_freq, memory_size):\n",
    "    memory = deque(maxlen=memory_size)  # Experience replay buffer\n",
    "    epsilon = epsilon_start\n",
    "    scores = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Select action using epsilon-greedy strategy\n",
    "            action = select_action(state, policy_net, epsilon, env.action_space.n)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = torch.tensor(next_state, dtype=torch.float32).unsqueeze(0)\n",
    "\n",
    "            # Store transition in memory\n",
    "            memory.append((state.numpy(), action, reward, next_state.numpy(), done))\n",
    "\n",
    "            # Update state and total reward\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "            # Optimize the model\n",
    "            optimize_model(policy_net, target_net, optimizer, memory, batch_size, gamma)\n",
    "\n",
    "        # Decay epsilon\n",
    "        epsilon = max(epsilon_end, epsilon_decay * epsilon)\n",
    "\n",
    "        # Update the target network every few episodes\n",
    "        if episode % target_update_freq == 0:\n",
    "            target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "        # Save total reward for tracking progress\n",
    "        scores.append(total_reward)\n",
    "\n",
    "        # Print progress every 50 episodes\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            print(f\"Episode {episode+1}/{num_episodes}, Total Reward: {total_reward}, Epsilon: {epsilon:.4f}\")\n",
    "\n",
    "    return scores\n",
    "\n",
    "# Function to simulate the agent after training\n",
    "def simulate_agent(env, policy_net, episodes=5):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            # Render the environment\n",
    "            img = env.render(mode='rgb_array')\n",
    "            plt.imshow(img)\n",
    "            plt.axis('off')\n",
    "            clear_output(wait=True)\n",
    "            plt.show()\n",
    "\n",
    "            # Choose action\n",
    "            state = torch.tensor(state, dtype=torch.float32).unsqueeze(0)\n",
    "            with torch.no_grad():\n",
    "                action = policy_net(state).argmax().item()\n",
    "\n",
    "            # Step the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "\n",
    "        print(f\"Episode {episode+1} finished with total reward: {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Main execution\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "env.seed(42)\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "# Initialize policy and target networks\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "policy_net = QNetwork(state_dim, action_dim)\n",
    "target_net = QNetwork(state_dim, action_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())  # Initialize target network with same weights\n",
    "target_net.eval()  # Target network doesn't get updated during backpropagation\n",
    "\n",
    "# Hyperparameters\n",
    "gamma = 0.99  # Discount factor\n",
    "epsilon_start = 1.0  # Initial epsilon value for epsilon-greedy strategy\n",
    "epsilon_end = 0.01  # Minimum epsilon value\n",
    "epsilon_decay = 0.995  # Epsilon decay rate\n",
    "target_update_freq = 10  # Update target network every 10 episodes\n",
    "num_episodes = 500  # Number of episodes for training\n",
    "batch_size = 64  # Batch size for experience replay\n",
    "memory_size = 10000  # Size of experience replay buffer\n",
    "learning_rate = 0.001  # Learning rate\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the agent using DQN\n",
    "scores = train_dqn(env, policy_net, target_net, optimizer, num_episodes, batch_size, gamma, epsilon_start, epsilon_end, epsilon_decay, target_update_freq, memory_size)\n",
    "\n",
    "# Plot the training progress\n",
    "plt.plot(scores)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN on CartPole-v0')\n",
    "plt.show()\n",
    "\n",
    "# Simulate the trained agent\n",
    "simulate_agent(env, policy_net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
