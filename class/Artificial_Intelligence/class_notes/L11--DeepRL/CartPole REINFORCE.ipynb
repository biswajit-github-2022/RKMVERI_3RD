{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 finished with total reward: 200.0\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Define the policy network\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim, hidden_dim=128):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, action_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.relu(self.fc1(state))\n",
    "        action_probs = self.softmax(self.fc2(x))\n",
    "        return action_probs\n",
    "\n",
    "# Function to compute discounted rewards\n",
    "def compute_discounted_rewards(rewards, gamma):\n",
    "    discounted_rewards = []\n",
    "    running_add = 0\n",
    "    #These rewards are in chronological order, from the first time step to the last.\n",
    "    for r in reversed(rewards): \n",
    "        running_add = r + gamma * running_add\n",
    "        #inserted at the beginning to maintain correct order\n",
    "        discounted_rewards.insert(0, running_add) \n",
    "    return discounted_rewards\n",
    "\n",
    "# Function to render CartPole-v0 environment in notebook\n",
    "def render_env(env):\n",
    "    img = env.render(mode='rgb_array')\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    clear_output(wait=True)\n",
    "    plt.show()\n",
    "\n",
    "# Function to train using REINFORCE algorithm\n",
    "def train_reinforce(env, policy, optimizer, gamma=0.99, max_episodes=500, max_timesteps=200):\n",
    "    episode_rewards = []\n",
    "    for episode in range(max_episodes):\n",
    "        state = env.reset()\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        total_reward = 0\n",
    "\n",
    "        for t in range(max_timesteps):\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            action_probs = policy(state_tensor)\n",
    "            \n",
    "            # Sample an action from the policy's action distribution\n",
    "            action = np.random.choice(np.array([0, 1]), p=action_probs.detach().numpy().squeeze())\n",
    "            log_prob = torch.log(action_probs.squeeze(0)[action])\n",
    "            log_probs.append(log_prob)\n",
    "            \n",
    "            # Step the environment\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            rewards.append(reward)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # Compute discounted rewards\n",
    "        discounted_rewards = compute_discounted_rewards(rewards, gamma)\n",
    "        discounted_rewards = torch.tensor(discounted_rewards)\n",
    "\n",
    "        # Normalize the rewards\n",
    "        discounted_rewards = (discounted_rewards - discounted_rewards.mean()) / (discounted_rewards.std() + 1e-8)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = torch.stack([-log_prob * reward for log_prob, reward in zip(log_probs, discounted_rewards)]).sum()\n",
    "\n",
    "        # Perform backpropagation and update policy parameters\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Append total reward for this episode\n",
    "        episode_rewards.append(total_reward)\n",
    "\n",
    "        # Print progress every 50 episodes\n",
    "        if (episode + 1) % 50 == 0:\n",
    "            print(f'Episode {episode+1}, Total Reward: {total_reward}')\n",
    "\n",
    "    return episode_rewards\n",
    "\n",
    "# Simulate the trained agent\n",
    "def simulate_agent(env, policy, episodes=5):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            render_env(env)\n",
    "            \n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            action_probs = policy(state_tensor)\n",
    "            action = np.random.choice(np.array([0, 1]), p=action_probs.detach().numpy().squeeze())\n",
    "            \n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "\n",
    "        print(f\"Episode {episode+1} finished with total reward: {total_reward}\")\n",
    "\n",
    "    env.close()\n",
    "\n",
    "# Set up CartPole-v0 environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Define policy network and optimizer\n",
    "state_dim = env.observation_space.shape[0] \n",
    "#For CartPole-v0, this is 4, \n",
    "#as the state consists of 4 variables (cart position, cart velocity, pole angle, and pole velocity).\n",
    "action_dim = env.action_space.n # two actions: move the cart left or right\n",
    "policy = PolicyNetwork(state_dim, action_dim)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=0.001)\n",
    "\n",
    "# Train the model using REINFORCE\n",
    "episode_rewards = train_reinforce(env, policy, optimizer)\n",
    "\n",
    "\n",
    "# Plot the training progress\n",
    "plt.plot(episode_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('REINFORCE on CartPole-v0')\n",
    "plt.show()\n",
    "\n",
    "# Simulate the trained agent\n",
    "simulate_agent(env, policy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state1 = env.reset()\n",
    "action = env.action_space.sample()\n",
    "state, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "l1 = 4\n",
    "l2 = 150\n",
    "l3 = 2\n",
    "model = torch.nn.Sequential(\n",
    "    torch.nn.Linear(l1, l2),\n",
    "    torch.nn.LeakyReLU(),\n",
    "    torch.nn.Linear(l2, l3),\n",
    "    torch.nn.Softmax(dim=-1)\n",
    ")\n",
    "learning_rate = 0.0009\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model(torch.from_numpy(state1).float())\n",
    "action = np.random.choice(np.array([0,1]), p=pred.data.numpy())\n",
    "state2, reward, done, info = env.step(action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_rewards(rewards, gamma=0.99):\n",
    "    lenr = len(rewards)\n",
    "    disc_return = torch.pow(gamma,torch.arange(lenr).float()) * rewards\n",
    "    disc_return /= disc_return.max()\n",
    "    return disc_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(preds, r):\n",
    "    return -1 * torch.sum(r * torch.log(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DUR = 200\n",
    "MAX_EPISODES = 500\n",
    "gamma = 0.99\n",
    "score = []\n",
    "\n",
    "for episode in range(MAX_EPISODES):\n",
    "    curr_state = env.reset()\n",
    "    done = False\n",
    "    transitions = []\n",
    "    \n",
    "    for t in range(MAX_DUR):\n",
    "        act_prob = model(torch.from_numpy(curr_state).float())  # Get action probabilities\n",
    "        action = np.random.choice(np.array([0, 1]), p=act_prob.data.numpy())  # Sample an action\n",
    "        prev_state = curr_state\n",
    "        curr_state, _, done, _ = env.step(action)  # Execute action in environment\n",
    "        \n",
    "        transitions.append((prev_state, action, t + 1))  # Store transition\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "    \n",
    "        ep_len = len(transitions)\n",
    "        score.append(ep_len)\n",
    "\n",
    "        # Flip rewards for calculating discounted rewards\n",
    "        reward_batch = torch.Tensor([r for (s, a, r) in transitions]).flip(dims=(0,))\n",
    "\n",
    "        # Calculate discounted rewards\n",
    "        disc_rewards = discount_rewards(reward_batch)\n",
    "\n",
    "        # Create state, action batches\n",
    "        \n",
    "        # Create state, action batches by first converting to numpy array\n",
    "        state_batch = torch.Tensor(np.array([s for (s, a, r) in transitions]))  # Convert to numpy array first\n",
    "        action_batch = torch.Tensor(np.array([a for (s, a, r) in transitions]))  # Convert to numpy array first\n",
    "\n",
    "        # Predict action probabilities from the model\n",
    "        pred_batch = model(state_batch)  # Should be of shape (batch_size, action_dim)\n",
    "\n",
    "        # Use `gather` to select probabilities of the taken actions\n",
    "        prob_batch = pred_batch.gather(dim=1, index=action_batch.long().view(-1, 1)).squeeze()\n",
    "\n",
    "        # Calculate the loss (assumed loss_fn is defined as a negative log likelihood or similar)\n",
    "        loss = loss_fn(prob_batch, disc_rewards)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def render_env(env):\n",
    "    # Get the image from the environment and display it in the notebook\n",
    "    img = env.render(mode='rgb_array')  # Capture the current frame as an image\n",
    "    plt.imshow(img)  # Use matplotlib to render the image\n",
    "    plt.axis('off')  # Hide axis\n",
    "    clear_output(wait=True)  # Clear the previous frame output\n",
    "    plt.show()  # Show the current frame\n",
    "\n",
    "def watch_agent_in_action(env, model, episodes=5):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            render_env(env)  # Render environment in notebook\n",
    "            \n",
    "            # Get action probabilities from the trained model\n",
    "            state_tensor = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            action_probs = model(state_tensor)\n",
    "            \n",
    "            # Select action based on the model's probabilities\n",
    "            action = np.random.choice(np.array([0, 1]), p=action_probs.detach().numpy().squeeze())\n",
    "            \n",
    "            # Perform the action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "        print(f\"Episode {episode+1} finished with total reward: {total_reward}\")\n",
    "    env.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 5 finished with total reward: 193.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "#model = PolicyNetwork(state_dim, action_dim)\n",
    "\n",
    "# After training, visualize the agent in action\n",
    "print(\"Training complete. Watching the agent in action...\")\n",
    "watch_agent_in_action(env, model, episodes=5)  # Run for 5 episodes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DONT USE THE BELOW SECTION FOR NOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def watch_agent_in_action(env, model, episodes=5):\n",
    "    for episode in range(episodes):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            env.render()  # Render the environment\n",
    "            \n",
    "            # Get action probabilities from the trained model\n",
    "            state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "            action_probs = model(state)\n",
    "            \n",
    "            # Select action based on the model's probabilities\n",
    "            action = np.random.choice(np.array([0, 1]), p=action_probs.detach().numpy().squeeze())\n",
    "            \n",
    "            # Perform the action\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            total_reward += reward\n",
    "            state = next_state\n",
    "            \n",
    "        print(f\"Episode {episode+1} finished with total reward: {total_reward}\")\n",
    "    env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
