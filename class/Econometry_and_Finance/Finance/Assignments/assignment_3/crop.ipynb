{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CROP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output directories\n",
    "input_directory = \"data\"  # Replace with your actual input directory\n",
    "output_directory = \"cropped\"  # Replace with your actual output directory\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Target date for filtering\n",
    "filter_date = pd.to_datetime('2019-04-30')\n",
    "\n",
    "# Iterate over each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Convert the \"Date\" column to datetime\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            \n",
    "            # Filter the DataFrame for dates on or after '2019-04-30'\n",
    "            filtered_df = df[df['Date'] >= filter_date].loc[:, [\"Date\", \"Open\", \"High\", \"Low\", \"Close\"]]\n",
    "            \n",
    "            # Create a new file name for the output\n",
    "            output_file_path = os.path.join(output_directory, f\"{filename}\")\n",
    "            \n",
    "            # Save the filtered DataFrame to a new CSV file\n",
    "            filtered_df.to_csv(output_file_path, index=False)\n",
    "            print(f\"Processed and saved: {output_file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during processing\n",
    "            print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop 1.5 Year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: cropped_2\\ADANIPORTS.csv\n",
      "Processed and saved: cropped_2\\ASIANPAINT.csv\n",
      "Processed and saved: cropped_2\\AXISBANK.csv\n",
      "Processed and saved: cropped_2\\BAJAJ-AUTO.csv\n",
      "Processed and saved: cropped_2\\BAJAJFINSV.csv\n",
      "Processed and saved: cropped_2\\BAJFINANCE.csv\n",
      "Processed and saved: cropped_2\\BHARTIARTL.csv\n",
      "Processed and saved: cropped_2\\BPCL.csv\n",
      "Processed and saved: cropped_2\\BRITANNIA.csv\n",
      "Processed and saved: cropped_2\\CIPLA.csv\n",
      "Processed and saved: cropped_2\\COALINDIA.csv\n",
      "Processed and saved: cropped_2\\DRREDDY.csv\n",
      "Processed and saved: cropped_2\\EICHERMOT.csv\n",
      "Processed and saved: cropped_2\\GAIL.csv\n",
      "Processed and saved: cropped_2\\GRASIM.csv\n",
      "Processed and saved: cropped_2\\HCLTECH.csv\n",
      "Processed and saved: cropped_2\\HDFC.csv\n",
      "Processed and saved: cropped_2\\HDFCBANK.csv\n",
      "Processed and saved: cropped_2\\HEROMOTOCO.csv\n",
      "Processed and saved: cropped_2\\HINDALCO.csv\n",
      "Processed and saved: cropped_2\\HINDUNILVR.csv\n",
      "Processed and saved: cropped_2\\ICICIBANK.csv\n",
      "Processed and saved: cropped_2\\INDUSINDBK.csv\n",
      "Processed and saved: cropped_2\\INFY.csv\n",
      "Processed and saved: cropped_2\\IOC.csv\n",
      "Processed and saved: cropped_2\\ITC.csv\n",
      "Processed and saved: cropped_2\\JSWSTEEL.csv\n",
      "Processed and saved: cropped_2\\KOTAKBANK.csv\n",
      "Processed and saved: cropped_2\\LT.csv\n",
      "Processed and saved: cropped_2\\MARUTI.csv\n",
      "Processed and saved: cropped_2\\MM.csv\n",
      "Processed and saved: cropped_2\\NESTLEIND.csv\n",
      "Processed and saved: cropped_2\\NIFTY50_all.csv\n",
      "Processed and saved: cropped_2\\NTPC.csv\n",
      "Processed and saved: cropped_2\\ONGC.csv\n",
      "Processed and saved: cropped_2\\POWERGRID.csv\n",
      "Processed and saved: cropped_2\\RELIANCE.csv\n",
      "Processed and saved: cropped_2\\SBIN.csv\n",
      "Processed and saved: cropped_2\\SHREECEM.csv\n",
      "Processed and saved: cropped_2\\SUNPHARMA.csv\n",
      "Processed and saved: cropped_2\\TATAMOTORS.csv\n",
      "Processed and saved: cropped_2\\TATASTEEL.csv\n",
      "Processed and saved: cropped_2\\TCS.csv\n",
      "Processed and saved: cropped_2\\TECHM.csv\n",
      "Processed and saved: cropped_2\\TITAN.csv\n",
      "Processed and saved: cropped_2\\ULTRACEMCO.csv\n",
      "Processed and saved: cropped_2\\UPL.csv\n",
      "Processed and saved: cropped_2\\VEDL.csv\n",
      "Processed and saved: cropped_2\\WIPRO.csv\n",
      "Processed and saved: cropped_2\\ZEEL.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output directories\n",
    "input_directory = \"data\"  # Replace with your actual input directory\n",
    "output_directory = \"cropped_2\"  # Replace with your actual output directory\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Target date range for filtering\n",
    "filter_date1 = pd.to_datetime('2019-04-30')\n",
    "filter_date2 = pd.to_datetime('2020-10-30')\n",
    "\n",
    "# Iterate over each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Convert the \"Date\" column to datetime\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            \n",
    "            # Filter the DataFrame for dates between '2019-04-30' and '2020-10-30'\n",
    "            filtered_df = df[(df['Date'] >= filter_date1) & (df['Date'] <= filter_date2)].loc[:, [\"Date\", \"Open\", \"High\", \"Low\", \"Close\"]]\n",
    "            \n",
    "            # Create a new file name for the output\n",
    "            output_file_path = os.path.join(output_directory, filename)\n",
    "            \n",
    "            # Save the filtered DataFrame to a new CSV file\n",
    "            filtered_df.to_csv(output_file_path, index=False)\n",
    "            print(f\"Processed and saved: {output_file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during processing\n",
    "            print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: return_2\\ADANIPORTS.csv\n",
      "Processed and saved: return_2\\ASIANPAINT.csv\n",
      "Processed and saved: return_2\\AXISBANK.csv\n",
      "Processed and saved: return_2\\BAJAJ-AUTO.csv\n",
      "Processed and saved: return_2\\BAJAJFINSV.csv\n",
      "Processed and saved: return_2\\BAJFINANCE.csv\n",
      "Processed and saved: return_2\\BHARTIARTL.csv\n",
      "Processed and saved: return_2\\BPCL.csv\n",
      "Processed and saved: return_2\\BRITANNIA.csv\n",
      "Processed and saved: return_2\\CIPLA.csv\n",
      "Processed and saved: return_2\\COALINDIA.csv\n",
      "Processed and saved: return_2\\DRREDDY.csv\n",
      "Processed and saved: return_2\\EICHERMOT.csv\n",
      "Processed and saved: return_2\\GAIL.csv\n",
      "Processed and saved: return_2\\GRASIM.csv\n",
      "Processed and saved: return_2\\HCLTECH.csv\n",
      "Processed and saved: return_2\\HDFC.csv\n",
      "Processed and saved: return_2\\HDFCBANK.csv\n",
      "Processed and saved: return_2\\HEROMOTOCO.csv\n",
      "Processed and saved: return_2\\HINDALCO.csv\n",
      "Processed and saved: return_2\\HINDUNILVR.csv\n",
      "Processed and saved: return_2\\ICICIBANK.csv\n",
      "Processed and saved: return_2\\INDUSINDBK.csv\n",
      "Processed and saved: return_2\\INFY.csv\n",
      "Processed and saved: return_2\\IOC.csv\n",
      "Processed and saved: return_2\\ITC.csv\n",
      "Processed and saved: return_2\\JSWSTEEL.csv\n",
      "Processed and saved: return_2\\KOTAKBANK.csv\n",
      "Processed and saved: return_2\\LT.csv\n",
      "Processed and saved: return_2\\MARUTI.csv\n",
      "Processed and saved: return_2\\MM.csv\n",
      "Processed and saved: return_2\\NESTLEIND.csv\n",
      "Processed and saved: return_2\\NIFTY50_all.csv\n",
      "Processed and saved: return_2\\NTPC.csv\n",
      "Processed and saved: return_2\\ONGC.csv\n",
      "Processed and saved: return_2\\POWERGRID.csv\n",
      "Processed and saved: return_2\\RELIANCE.csv\n",
      "Processed and saved: return_2\\SBIN.csv\n",
      "Processed and saved: return_2\\SHREECEM.csv\n",
      "Processed and saved: return_2\\SUNPHARMA.csv\n",
      "Processed and saved: return_2\\TATAMOTORS.csv\n",
      "Processed and saved: return_2\\TATASTEEL.csv\n",
      "Processed and saved: return_2\\TCS.csv\n",
      "Processed and saved: return_2\\TECHM.csv\n",
      "Processed and saved: return_2\\TITAN.csv\n",
      "Processed and saved: return_2\\ULTRACEMCO.csv\n",
      "Processed and saved: return_2\\UPL.csv\n",
      "Processed and saved: return_2\\VEDL.csv\n",
      "Processed and saved: return_2\\WIPRO.csv\n",
      "Processed and saved: return_2\\ZEEL.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directories\n",
    "filtered_directory = \"cropped_2\"  # Replace with your actual filtered files directory\n",
    "output_directory = \"return_2\"      # Replace with your actual output directory\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to calculate \"Return\" and create a new CSV file\n",
    "def calculate_return(df):\n",
    "    # Sort the DataFrame by Date to ensure proper order\n",
    "    df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    # Calculate the \"Return\" as (current Close - previous Close) / previous Close\n",
    "    df['Return'] = df['Close'].pct_change()\n",
    "\n",
    "    # Keep only the \"Date\" and \"Return\" columns\n",
    "    return_df = df[['Date', 'Return']].dropna()\n",
    "\n",
    "    return return_df\n",
    "\n",
    "# Iterate over each filtered CSV file\n",
    "for filename in os.listdir(filtered_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(filtered_directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Read the filtered CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Calculate the return and create a new DataFrame\n",
    "            return_df = calculate_return(df)\n",
    "            \n",
    "            # Create a new file name for the output\n",
    "            output_file_path = os.path.join(output_directory, f\"{filename}\")\n",
    "            \n",
    "            # Save the new DataFrame to a CSV file\n",
    "            return_df.to_csv(output_file_path, index=False)\n",
    "            print(f\"Processed and saved: {output_file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during processing\n",
    "            print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADANIPORTS\n",
      "ASIANPAINT\n",
      "AXISBANK\n",
      "BAJAJ-AUTO\n",
      "BAJAJFINSV\n",
      "BAJFINANCE\n",
      "BHARTIARTL\n",
      "BPCL\n",
      "BRITANNIA\n",
      "CIPLA\n",
      "COALINDIA\n",
      "DRREDDY\n",
      "EICHERMOT\n",
      "GAIL\n",
      "GRASIM\n",
      "HCLTECH\n",
      "HDFC\n",
      "HDFCBANK\n",
      "HEROMOTOCO\n",
      "HINDALCO\n",
      "HINDUNILVR\n",
      "ICICIBANK\n",
      "INDUSINDBK\n",
      "INFY\n",
      "IOC\n",
      "ITC\n",
      "JSWSTEEL\n",
      "KOTAKBANK\n",
      "LT\n",
      "MARUTI\n",
      "MM\n",
      "NESTLEIND\n",
      "NIFTY50_all\n",
      "NTPC\n",
      "ONGC\n",
      "POWERGRID\n",
      "RELIANCE\n",
      "SBIN\n",
      "SHREECEM\n",
      "SUNPHARMA\n",
      "TATAMOTORS\n",
      "TATASTEEL\n",
      "TCS\n",
      "TECHM\n",
      "TITAN\n",
      "ULTRACEMCO\n",
      "UPL\n",
      "VEDL\n",
      "WIPRO\n",
      "ZEEL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the files\n",
    "directory = \"return_2\"  # Replace with the actual directory path\n",
    "\n",
    "# List to store file names without extensions\n",
    "file_names = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if it's a file (not a directory)\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        # Split the file name and extension, and store the file name without extension\n",
    "        name, _ = os.path.splitext(filename)\n",
    "        file_names.append(name)\n",
    "\n",
    "# Print the list of file names without extensions\n",
    "for name in file_names:\n",
    "    print(name)\n",
    "\n",
    "# file_names\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check shape of all return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ADANIPORTS': (374, 2), 'ASIANPAINT': (374, 2), 'AXISBANK': (374, 2), 'BAJAJ-AUTO': (374, 2), 'BAJAJFINSV': (374, 2), 'BAJFINANCE': (374, 2), 'BHARTIARTL': (374, 2), 'BPCL': (374, 2), 'BRITANNIA': (374, 2), 'CIPLA': (374, 2), 'COALINDIA': (374, 2), 'DRREDDY': (374, 2), 'EICHERMOT': (374, 2), 'GAIL': (374, 2), 'GRASIM': (374, 2), 'HCLTECH': (374, 2), 'HDFC': (374, 2), 'HDFCBANK': (374, 2), 'HEROMOTOCO': (374, 2), 'HINDALCO': (374, 2), 'HINDUNILVR': (374, 2), 'ICICIBANK': (374, 2), 'INDUSINDBK': (374, 2), 'INFY': (374, 2), 'IOC': (374, 2), 'ITC': (374, 2), 'JSWSTEEL': (374, 2), 'KOTAKBANK': (374, 2), 'LT': (374, 2), 'MARUTI': (374, 2), 'MM': (374, 2), 'NESTLEIND': (374, 2), 'NIFTY50_all': (18374, 2), 'NTPC': (374, 2), 'ONGC': (374, 2), 'POWERGRID': (374, 2), 'RELIANCE': (374, 2), 'SBIN': (374, 2), 'SHREECEM': (374, 2), 'SUNPHARMA': (374, 2), 'TATAMOTORS': (374, 2), 'TATASTEEL': (374, 2), 'TCS': (374, 2), 'TECHM': (374, 2), 'TITAN': (374, 2), 'ULTRACEMCO': (374, 2), 'UPL': (374, 2), 'VEDL': (374, 2), 'WIPRO': (374, 2), 'ZEEL': (374, 2)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the CSV files\n",
    "directory = \"return_2\"  # Replace with the actual directory path\n",
    "\n",
    "# Dictionary to store the shape of each DataFrame\n",
    "file_shapes = {}\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Get the file name without extension\n",
    "            file_name_without_ext = os.path.splitext(filename)[0]\n",
    "            \n",
    "            # Store the shape (rows, columns) of the DataFrame\n",
    "            file_shapes[file_name_without_ext] = df.shape\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Print the dictionary of file shapes\n",
    "print(file_shapes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the combined REturn Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined DataFrame: (374, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the directory containing the CSV files\n",
    "input_directory = 'selected_2'  # Replace with your actual directory path\n",
    "\n",
    "# List all CSV files in the directory\n",
    "files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize a list to store the \"Return\" columns\n",
    "return_columns = []\n",
    "\n",
    "# Loop through each file and extract the \"Return\" column\n",
    "for file in files:\n",
    "    file_path = os.path.join(input_directory, file)\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the \"Return\" column exists\n",
    "        if 'Return' in df.columns:\n",
    "            # Append the \"Return\" column to the list\n",
    "            return_columns.append(df[['Return']])\n",
    "        else:\n",
    "            print(f\"Warning: 'Return' column not found in {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# Concatenate the \"Return\" columns horizontally\n",
    "combined_df = pd.concat(return_columns, axis=1)\n",
    "\n",
    "# Print the shape of the combined DataFrame\n",
    "print(f\"Shape of combined DataFrame: {combined_df.shape}\")\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file (optional)\n",
    "output_file = 'selected_2/combined_returns.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now Goto the main.R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After getting weights paste them here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [8.948917e-02, 1.150844e-01, 1.003946e-01, 1.750287e-01, 3.027476e-02, 7.418727e-03, \n",
    "           1.449852e-18, 2.056144e-01, 2.766953e-01, 0.0]\n",
    "\n",
    "# Now `numbers` is a Python list containing the values.\n",
    "money_to_invest= 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "money_to_invest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "money_to_all=[]\n",
    "for i in weights:\n",
    "    money_to_all.append(money_to_invest*i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[89489.17000000001,\n",
       " 115084.40000000001,\n",
       " 100394.6,\n",
       " 175028.7,\n",
       " 30274.760000000002,\n",
       " 7418.727,\n",
       " 1.449852e-12,\n",
       " 205614.4,\n",
       " 276695.3,\n",
       " 0.0]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "money_to_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['BRITANNIA',\n",
       " 'CIPLA',\n",
       " 'COALINDIA',\n",
       " 'combined_returns',\n",
       " 'DRREDDY',\n",
       " 'HDFCBANK',\n",
       " 'HEROMOTOCO',\n",
       " 'ICICIBANK',\n",
       " 'NESTLEIND',\n",
       " 'POWERGRID',\n",
       " 'TATAMOTORS']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the files\n",
    "directory = \"selected_2\"  # Replace with the actual directory path\n",
    "\n",
    "# List to store file names without extensions\n",
    "file_names = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if it's a file (not a directory)\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        # Split the file name and extension, and store the file name without extension\n",
    "        name, _ = os.path.splitext(filename)\n",
    "        file_names.append(name)\n",
    "\n",
    "# Print the list of file names without extensions\n",
    "# for name in file_names:\n",
    "#     print(name)\n",
    "\n",
    "file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Close values on 2020-05-03: [3473.25, 754.5, 114.2, 4888.65, 1183.55, 2799.8, 392.6, 17161.6, 171.0, 132.65]\n",
      "Close values on 2021-04-30: [3449.0, 910.35, 133.05, 5163.1, 1412.3, 2819.15, 600.5, 16309.25, 220.05, 293.85]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Folder containing your CSV files\n",
    "input_directory = \"data\"  # Replace with your actual input directory\n",
    "\n",
    "# List of CSV files to select\n",
    "selected_files = [\n",
    "    'BRITANNIA', 'CIPLA', 'COALINDIA', 'DRREDDY', \n",
    "    'HDFCBANK', 'HEROMOTOCO', 'ICICIBANK', 'NESTLEIND', 'POWERGRID', 'TATAMOTORS'\n",
    "]\n",
    "\n",
    "# Dates to select\n",
    "date1 = pd.to_datetime(\"2020-10-30\")\n",
    "date2 = pd.to_datetime(\"2021-04-30\")\n",
    "\n",
    "# Lists to store \"Close\" values\n",
    "close_values_date1 = []\n",
    "close_values_date2 = []\n",
    "\n",
    "# Iterate over each selected file\n",
    "for file_name in selected_files:\n",
    "    file_path = os.path.join(input_directory, f\"{file_name}.csv\")\n",
    "    \n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Convert the \"Date\" column to datetime\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Get \"Close\" values for the specific dates\n",
    "        close_value_date1 = df.loc[df['Date'] == date1, 'Close'].values\n",
    "        close_value_date2 = df.loc[df['Date'] == date2, 'Close'].values\n",
    "        \n",
    "        # Check if the date exists in the DataFrame and add to lists\n",
    "        if close_value_date1.size > 0:\n",
    "            close_values_date1.append(close_value_date1[0])\n",
    "        else:\n",
    "            close_values_date1.append(None)  # Append None if the date is not found\n",
    "        \n",
    "        if close_value_date2.size > 0:\n",
    "            close_values_date2.append(close_value_date2[0])\n",
    "        else:\n",
    "            close_values_date2.append(None)  # Append None if the date is not found\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {file_name}: {e}\")\n",
    "\n",
    "# Output the lists\n",
    "print(\"Close values on 2020-05-03:\", close_values_date1)\n",
    "print(\"Close values on 2021-04-30:\", close_values_date2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[25.0, 152.0, 879.0, 35.0, 25.0, 2.0, 0.0, 11.0, 1618.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Element-wise integer division\n",
    "result = [a // b for a, b in zip(money_to_all,close_values_date1)]\n",
    "\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[86225.0, 138373.2, 116950.95000000001, 180708.5, 35307.5, 5638.3, 0.0, 179401.75, 356040.9, 0.0]\n"
     ]
    }
   ],
   "source": [
    "# Element-wise integer division\n",
    "result2 = [a * b for a, b in zip(result,close_values_date2)]\n",
    "\n",
    "print(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1098646.1"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98646.1000000001"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(result2) - money_to_invest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
