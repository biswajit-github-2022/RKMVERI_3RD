{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error processing ADANIPORTS.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing ASIANPAINT.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing AXISBANK.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing BAJAJ-AUTO.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing BAJAJFINSV.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing BAJFINANCE.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing BHARTIARTL.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing BPCL.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing BRITANNIA.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing CIPLA.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing COALINDIA.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing DRREDDY.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing EICHERMOT.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing GAIL.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing GRASIM.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing HCLTECH.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing HDFC.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing HDFCBANK.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing HEROMOTOCO.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing HINDALCO.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing HINDUNILVR.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing ICICIBANK.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing INDUSINDBK.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing INFY.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing IOC.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing ITC.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing JSWSTEEL.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing KOTAKBANK.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing LT.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing MARUTI.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing MM.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing NESTLEIND.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing NIFTY50_all.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing NTPC.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing ONGC.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing POWERGRID.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing RELIANCE.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing SBIN.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing SHREECEM.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing SUNPHARMA.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing TATAMOTORS.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing TATASTEEL.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing TCS.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing TECHM.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing TITAN.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing ULTRACEMCO.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing UPL.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing VEDL.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing WIPRO.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
      "Error processing ZEEL.csv: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output directories\n",
    "input_directory = \"data\"  # Replace with your actual input directory\n",
    "output_directory = \"cropped\"  # Replace with your actual output directory\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Target date for filtering\n",
    "filter_date = pd.to_datetime('2019-04-30')\n",
    "\n",
    "# Iterate over each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Convert the \"Date\" column to datetime\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            \n",
    "            # Filter the DataFrame for dates on or after '2019-04-30'\n",
    "            filtered_df = df[df['Date'] >= filter_date].loc[:, [\"Date\", \"Open\", \"High\", \"Low\", \"Close\"]]\n",
    "            \n",
    "            # Create a new file name for the output\n",
    "            output_file_path = os.path.join(output_directory, f\"{filename}\")\n",
    "            \n",
    "            # Save the filtered DataFrame to a new CSV file\n",
    "            filtered_df.to_csv(output_file_path, index=False)\n",
    "            print(f\"Processed and saved: {output_file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during processing\n",
    "            print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: cropped_2\\ADANIPORTS.csv\n",
      "Processed and saved: cropped_2\\ASIANPAINT.csv\n",
      "Processed and saved: cropped_2\\AXISBANK.csv\n",
      "Processed and saved: cropped_2\\BAJAJ-AUTO.csv\n",
      "Processed and saved: cropped_2\\BAJAJFINSV.csv\n",
      "Processed and saved: cropped_2\\BAJFINANCE.csv\n",
      "Processed and saved: cropped_2\\BHARTIARTL.csv\n",
      "Processed and saved: cropped_2\\BPCL.csv\n",
      "Processed and saved: cropped_2\\BRITANNIA.csv\n",
      "Processed and saved: cropped_2\\CIPLA.csv\n",
      "Processed and saved: cropped_2\\COALINDIA.csv\n",
      "Processed and saved: cropped_2\\DRREDDY.csv\n",
      "Processed and saved: cropped_2\\EICHERMOT.csv\n",
      "Processed and saved: cropped_2\\GAIL.csv\n",
      "Processed and saved: cropped_2\\GRASIM.csv\n",
      "Processed and saved: cropped_2\\HCLTECH.csv\n",
      "Processed and saved: cropped_2\\HDFC.csv\n",
      "Processed and saved: cropped_2\\HDFCBANK.csv\n",
      "Processed and saved: cropped_2\\HEROMOTOCO.csv\n",
      "Processed and saved: cropped_2\\HINDALCO.csv\n",
      "Processed and saved: cropped_2\\HINDUNILVR.csv\n",
      "Processed and saved: cropped_2\\ICICIBANK.csv\n",
      "Processed and saved: cropped_2\\INDUSINDBK.csv\n",
      "Processed and saved: cropped_2\\INFY.csv\n",
      "Processed and saved: cropped_2\\IOC.csv\n",
      "Processed and saved: cropped_2\\ITC.csv\n",
      "Processed and saved: cropped_2\\JSWSTEEL.csv\n",
      "Processed and saved: cropped_2\\KOTAKBANK.csv\n",
      "Processed and saved: cropped_2\\LT.csv\n",
      "Processed and saved: cropped_2\\MARUTI.csv\n",
      "Processed and saved: cropped_2\\MM.csv\n",
      "Processed and saved: cropped_2\\NESTLEIND.csv\n",
      "Processed and saved: cropped_2\\NIFTY50_all.csv\n",
      "Processed and saved: cropped_2\\NTPC.csv\n",
      "Processed and saved: cropped_2\\ONGC.csv\n",
      "Processed and saved: cropped_2\\POWERGRID.csv\n",
      "Processed and saved: cropped_2\\RELIANCE.csv\n",
      "Processed and saved: cropped_2\\SBIN.csv\n",
      "Processed and saved: cropped_2\\SHREECEM.csv\n",
      "Processed and saved: cropped_2\\SUNPHARMA.csv\n",
      "Processed and saved: cropped_2\\TATAMOTORS.csv\n",
      "Processed and saved: cropped_2\\TATASTEEL.csv\n",
      "Processed and saved: cropped_2\\TCS.csv\n",
      "Processed and saved: cropped_2\\TECHM.csv\n",
      "Processed and saved: cropped_2\\TITAN.csv\n",
      "Processed and saved: cropped_2\\ULTRACEMCO.csv\n",
      "Processed and saved: cropped_2\\UPL.csv\n",
      "Processed and saved: cropped_2\\VEDL.csv\n",
      "Processed and saved: cropped_2\\WIPRO.csv\n",
      "Processed and saved: cropped_2\\ZEEL.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Input and output directories\n",
    "input_directory = \"data\"  # Replace with your actual input directory\n",
    "output_directory = \"cropped_2\"  # Replace with your actual output directory\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Target date range for filtering\n",
    "filter_date1 = pd.to_datetime('2019-04-30')\n",
    "filter_date2 = pd.to_datetime('2020-10-30')\n",
    "\n",
    "# Iterate over each file in the input directory\n",
    "for filename in os.listdir(input_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(input_directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Convert the \"Date\" column to datetime\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            \n",
    "            # Filter the DataFrame for dates between '2019-04-30' and '2020-10-30'\n",
    "            filtered_df = df[(df['Date'] >= filter_date1) & (df['Date'] <= filter_date2)].loc[:, [\"Date\", \"Open\", \"High\", \"Low\", \"Close\"]]\n",
    "            \n",
    "            # Create a new file name for the output\n",
    "            output_file_path = os.path.join(output_directory, filename)\n",
    "            \n",
    "            # Save the filtered DataFrame to a new CSV file\n",
    "            filtered_df.to_csv(output_file_path, index=False)\n",
    "            print(f\"Processed and saved: {output_file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during processing\n",
    "            print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed and saved: return_2\\ADANIPORTS.csv\n",
      "Processed and saved: return_2\\ASIANPAINT.csv\n",
      "Processed and saved: return_2\\AXISBANK.csv\n",
      "Processed and saved: return_2\\BAJAJ-AUTO.csv\n",
      "Processed and saved: return_2\\BAJAJFINSV.csv\n",
      "Processed and saved: return_2\\BAJFINANCE.csv\n",
      "Processed and saved: return_2\\BHARTIARTL.csv\n",
      "Processed and saved: return_2\\BPCL.csv\n",
      "Processed and saved: return_2\\BRITANNIA.csv\n",
      "Processed and saved: return_2\\CIPLA.csv\n",
      "Processed and saved: return_2\\COALINDIA.csv\n",
      "Processed and saved: return_2\\DRREDDY.csv\n",
      "Processed and saved: return_2\\EICHERMOT.csv\n",
      "Processed and saved: return_2\\GAIL.csv\n",
      "Processed and saved: return_2\\GRASIM.csv\n",
      "Processed and saved: return_2\\HCLTECH.csv\n",
      "Processed and saved: return_2\\HDFC.csv\n",
      "Processed and saved: return_2\\HDFCBANK.csv\n",
      "Processed and saved: return_2\\HEROMOTOCO.csv\n",
      "Processed and saved: return_2\\HINDALCO.csv\n",
      "Processed and saved: return_2\\HINDUNILVR.csv\n",
      "Processed and saved: return_2\\ICICIBANK.csv\n",
      "Processed and saved: return_2\\INDUSINDBK.csv\n",
      "Processed and saved: return_2\\INFY.csv\n",
      "Processed and saved: return_2\\IOC.csv\n",
      "Processed and saved: return_2\\ITC.csv\n",
      "Processed and saved: return_2\\JSWSTEEL.csv\n",
      "Processed and saved: return_2\\KOTAKBANK.csv\n",
      "Processed and saved: return_2\\LT.csv\n",
      "Processed and saved: return_2\\MARUTI.csv\n",
      "Processed and saved: return_2\\MM.csv\n",
      "Processed and saved: return_2\\NESTLEIND.csv\n",
      "Processed and saved: return_2\\NIFTY50_all.csv\n",
      "Processed and saved: return_2\\NTPC.csv\n",
      "Processed and saved: return_2\\ONGC.csv\n",
      "Processed and saved: return_2\\POWERGRID.csv\n",
      "Processed and saved: return_2\\RELIANCE.csv\n",
      "Processed and saved: return_2\\SBIN.csv\n",
      "Processed and saved: return_2\\SHREECEM.csv\n",
      "Processed and saved: return_2\\SUNPHARMA.csv\n",
      "Processed and saved: return_2\\TATAMOTORS.csv\n",
      "Processed and saved: return_2\\TATASTEEL.csv\n",
      "Processed and saved: return_2\\TCS.csv\n",
      "Processed and saved: return_2\\TECHM.csv\n",
      "Processed and saved: return_2\\TITAN.csv\n",
      "Processed and saved: return_2\\ULTRACEMCO.csv\n",
      "Processed and saved: return_2\\UPL.csv\n",
      "Processed and saved: return_2\\VEDL.csv\n",
      "Processed and saved: return_2\\WIPRO.csv\n",
      "Processed and saved: return_2\\ZEEL.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directories\n",
    "filtered_directory = \"cropped_2\"  # Replace with your actual filtered files directory\n",
    "output_directory = \"return_2\"      # Replace with your actual output directory\n",
    "\n",
    "# Ensure the output directory exists\n",
    "os.makedirs(output_directory, exist_ok=True)\n",
    "\n",
    "# Function to calculate \"Return\" and create a new CSV file\n",
    "def calculate_return(df):\n",
    "    # Sort the DataFrame by Date to ensure proper order\n",
    "    df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "    # Calculate the \"Return\" as (current Close - previous Close) / previous Close\n",
    "    df['Return'] = df['Close'].pct_change()\n",
    "\n",
    "    # Keep only the \"Date\" and \"Return\" columns\n",
    "    return_df = df[['Date', 'Return']].dropna()\n",
    "\n",
    "    return return_df\n",
    "\n",
    "# Iterate over each filtered CSV file\n",
    "for filename in os.listdir(filtered_directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        file_path = os.path.join(filtered_directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Read the filtered CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Calculate the return and create a new DataFrame\n",
    "            return_df = calculate_return(df)\n",
    "            \n",
    "            # Create a new file name for the output\n",
    "            output_file_path = os.path.join(output_directory, f\"{filename}\")\n",
    "            \n",
    "            # Save the new DataFrame to a CSV file\n",
    "            return_df.to_csv(output_file_path, index=False)\n",
    "            print(f\"Processed and saved: {output_file_path}\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            # Log any errors encountered during processing\n",
    "            print(f\"Error processing {filename}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ADANIPORTS\n",
      "ASIANPAINT\n",
      "AXISBANK\n",
      "BAJAJ-AUTO\n",
      "BAJAJFINSV\n",
      "BAJFINANCE\n",
      "BHARTIARTL\n",
      "BPCL\n",
      "BRITANNIA\n",
      "CIPLA\n",
      "COALINDIA\n",
      "DRREDDY\n",
      "EICHERMOT\n",
      "GAIL\n",
      "GRASIM\n",
      "HCLTECH\n",
      "HDFC\n",
      "HDFCBANK\n",
      "HEROMOTOCO\n",
      "HINDALCO\n",
      "HINDUNILVR\n",
      "ICICIBANK\n",
      "INDUSINDBK\n",
      "INFY\n",
      "IOC\n",
      "ITC\n",
      "JSWSTEEL\n",
      "KOTAKBANK\n",
      "LT\n",
      "MARUTI\n",
      "MM\n",
      "NESTLEIND\n",
      "NIFTY50_all\n",
      "NTPC\n",
      "ONGC\n",
      "POWERGRID\n",
      "RELIANCE\n",
      "SBIN\n",
      "SHREECEM\n",
      "SUNPHARMA\n",
      "TATAMOTORS\n",
      "TATASTEEL\n",
      "TCS\n",
      "TECHM\n",
      "TITAN\n",
      "ULTRACEMCO\n",
      "UPL\n",
      "VEDL\n",
      "WIPRO\n",
      "ZEEL\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Directory containing the files\n",
    "directory = \"return_2\"  # Replace with the actual directory path\n",
    "\n",
    "# List to store file names without extensions\n",
    "file_names = []\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    # Check if it's a file (not a directory)\n",
    "    if os.path.isfile(os.path.join(directory, filename)):\n",
    "        # Split the file name and extension, and store the file name without extension\n",
    "        name, _ = os.path.splitext(filename)\n",
    "        file_names.append(name)\n",
    "\n",
    "# Print the list of file names without extensions\n",
    "for name in file_names:\n",
    "    print(name)\n",
    "\n",
    "# file_names\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ADANIPORTS': (374, 2), 'ASIANPAINT': (374, 2), 'AXISBANK': (374, 2), 'BAJAJ-AUTO': (374, 2), 'BAJAJFINSV': (374, 2), 'BAJFINANCE': (374, 2), 'BHARTIARTL': (374, 2), 'BPCL': (374, 2), 'BRITANNIA': (374, 2), 'CIPLA': (374, 2), 'COALINDIA': (374, 2), 'DRREDDY': (374, 2), 'EICHERMOT': (374, 2), 'GAIL': (374, 2), 'GRASIM': (374, 2), 'HCLTECH': (374, 2), 'HDFC': (374, 2), 'HDFCBANK': (374, 2), 'HEROMOTOCO': (374, 2), 'HINDALCO': (374, 2), 'HINDUNILVR': (374, 2), 'ICICIBANK': (374, 2), 'INDUSINDBK': (374, 2), 'INFY': (374, 2), 'IOC': (374, 2), 'ITC': (374, 2), 'JSWSTEEL': (374, 2), 'KOTAKBANK': (374, 2), 'LT': (374, 2), 'MARUTI': (374, 2), 'MM': (374, 2), 'NESTLEIND': (374, 2), 'NIFTY50_all': (18374, 2), 'NTPC': (374, 2), 'ONGC': (374, 2), 'POWERGRID': (374, 2), 'RELIANCE': (374, 2), 'SBIN': (374, 2), 'SHREECEM': (374, 2), 'SUNPHARMA': (374, 2), 'TATAMOTORS': (374, 2), 'TATASTEEL': (374, 2), 'TCS': (374, 2), 'TECHM': (374, 2), 'TITAN': (374, 2), 'ULTRACEMCO': (374, 2), 'UPL': (374, 2), 'VEDL': (374, 2), 'WIPRO': (374, 2), 'ZEEL': (374, 2)}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Directory containing the CSV files\n",
    "directory = \"return_2\"  # Replace with the actual directory path\n",
    "\n",
    "# Dictionary to store the shape of each DataFrame\n",
    "file_shapes = {}\n",
    "\n",
    "# Iterate over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        # Construct full file path\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        try:\n",
    "            # Read the CSV file into a DataFrame\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Get the file name without extension\n",
    "            file_name_without_ext = os.path.splitext(filename)[0]\n",
    "            \n",
    "            # Store the shape (rows, columns) of the DataFrame\n",
    "            file_shapes[file_name_without_ext] = df.shape\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {filename}: {e}\")\n",
    "\n",
    "# Print the dictionary of file shapes\n",
    "print(file_shapes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of combined DataFrame: (374, 10)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Set the directory containing the CSV files\n",
    "input_directory = 'selected_2'  # Replace with your actual directory path\n",
    "\n",
    "# List all CSV files in the directory\n",
    "files = [f for f in os.listdir(input_directory) if f.endswith('.csv')]\n",
    "\n",
    "# Initialize a list to store the \"Return\" columns\n",
    "return_columns = []\n",
    "\n",
    "# Loop through each file and extract the \"Return\" column\n",
    "for file in files:\n",
    "    file_path = os.path.join(input_directory, file)\n",
    "    try:\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(file_path)\n",
    "        \n",
    "        # Check if the \"Return\" column exists\n",
    "        if 'Return' in df.columns:\n",
    "            # Append the \"Return\" column to the list\n",
    "            return_columns.append(df[['Return']])\n",
    "        else:\n",
    "            print(f\"Warning: 'Return' column not found in {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading {file}: {e}\")\n",
    "\n",
    "# Concatenate the \"Return\" columns horizontally\n",
    "combined_df = pd.concat(return_columns, axis=1)\n",
    "\n",
    "# Print the shape of the combined DataFrame\n",
    "print(f\"Shape of combined DataFrame: {combined_df.shape}\")\n",
    "\n",
    "# Save the combined DataFrame to a new CSV file (optional)\n",
    "output_file = 'selected_2/combined_returns.csv'\n",
    "combined_df.to_csv(output_file, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = [8.948917e-02, 1.150844e-01, 1.003946e-01, 1.750287e-01, 3.027476e-02, 7.418727e-03, \n",
    "           1.449852e-18, 2.056144e-01, 2.766953e-01, 0.0]\n",
    "\n",
    "# Now `numbers` is a Python list containing the values.\n",
    "money_to_invest= 1_000_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000000"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "money_to_invest"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
